## 1. 特征工程
https://zhuanlan.zhihu.com/p/111296130

###  1.0特征工程学习笔记
> Fire 2018.5

![fe](./img/fe.jpg)

### 1.0.1 探索性数据分析（EDA，Exploratory Data Analysis）

画图：

* [matplotlib](https://github.com/fire717/Python-Learner/tree/master/tools/matplotlib)
* [seaboan](https://github.com/fire717/Python-Learner/tree/master/tools/seaborn)

### 1.0.2数据预处理

#### 1.0.3 基础方法
* 数据清洗
	1. 去重
	2. 过滤（把过高/过低的反常值用平滑值替代）

* 缺失值处理
	1. 直接赋0/-1
	2. 根据经验直接赋值
	3. 用已有的均值/中位数等统计值
	4. 用一个单独的机器学习算法通过其他特征来预测缺失值
	
	LightGBM和XGBoost都能将NaN作为数据的一部分进行学习，所以不需要处理缺失值。

* 二值化
	
	特征的二值化处理是将数值型数据输出为布尔类型。其核心在于设定一个阈值，当样本书籍大于该阈值时，输出为1，小于等于该阈值时输出为0。我们通常使用preproccessing库的Binarizer类对数据进行二值化处理。

* 标准化 [标准化与归一化](https://blog.csdn.net/u012101561/article/details/72506273)
	1. 均值方差法
	2. z-score标准化
	3. StandardScaler标准化
	
	把数据放缩到同样的范围 SVM/NN影响很大 树模型影响小。不是什么时候都需要标准化，比如物理意义非常明确的经纬度，如果标准化，其本身的意义就会丢失。
	
* 归一化
	1. 最大最小归一化（最常用）
	2. 对数函数转换（log）
	3. 反余切转换

* 区间缩放
	1. sklearn.preprocessing.MaxAbsScaler - scikit-learn 0.18.1 documentation，将一列的数值，除以这一列的最大绝对值。不免疫outlier。	
	2. sklearn.preprocessing.MinMaxScaler - scikit-learn 0.18.1 documentation。不免疫outlier。

	
* 离散化
	1. one-hot
	2. 把数据按不同区间划分（等宽划分或等频划分）
	3. 聚类编码/按层次进行编码
	4. 平均数编码（mean encoding）：针对高基数类别特征的有监督编码。当一个类别特征列包括了极多不同类别时（如家庭地址，动辄上万）时，可以采用。
	5. 低频类别：有时会有一些类别，在训练集和测试集中总共只出现一次，例如特别偏僻的郊区地址。此时，保留其原有的自然数编码意义不大，不如将所有频数为1的类别合并到同一个新的类别下。
	6. hash编码成词向量
	
* 统计值

	包括max, min, mean, std等。python中用pandas库序列化数据后，可以得到数据的统计值。

* 压缩范围

	有些分类变量的少部分取值可能占据了90%的case，这种情况下可以采用预测模型、领域专家、或者简单的频率分布统计。具体问题具体分析，高频和低频都是需要特别处理的地方，抛弃效果不好时，可以考虑采样（高频）或上采样（低频），加权等等方法。

#### 1.0.4 实例
* 不平衡类别 
	1. 集成学习+阈值调整
	2. 多的类别过采样/少的类别欠采样来平衡分布[欠采样（undersampling）和过采样（oversampling）会对模型带来怎样的影响？](https://www.zhihu.com/question/269698662/answer/352279936)

* 时间特征
	1. 提取成年/月/日
	2. 根据活动按周期提取（每月/周）
	3. 尝试模型ARMA/RNN（脸书开源工具FBprofit）。
	4. 这是一年的第n天，这是一年的第n周，这是一周的第n天，etc
	5. 时间序列：把昨天的特征加入今天的特征，或者把和昨天相比，特征数值的改变量加入今天的特征。
	

### 1.0.5 特征构造

* 特征提取
	1. 相关领域专家知识（比如速度与加速度，时域（均值方差等）与频域（傅立叶变换）等）
	2. 深度学习自动学习特征
	3. 原始数据本身就是特征
	4. 实验/经验/发现
	5. 特征组合：如对用户id和用户特征最组合来获得较大的特征集

* 特征变换（普通加减乘除没有意义。）
	1. 不同阶的差分
	2. 傅立叶变换
	3. 多项式做组合特征（二次三次等）（sklearn.preprocessing.PolynomialFeatures - scikit-learn 0.18.1 documentation）对于树类模型没有多少意义
	4. 核方法
	5. 非正态分布转正太分布（log），平方，立方，根号...（但任何针对单独特征列的单调变换（如对数）：不适用于决策树类算法。对于决策树而言，X 、X^3 、X^5 之间没有差异， |X| 、 X^2 、 X^4 之间没有差异，除非发生了舍入误差。）
	6. 线性组合（linear combination）：仅适用于决策树以及基于决策树的ensemble（如gradient boosting, random forest），因为常见的axis-aligned split function不擅长捕获不同特征之间的相关性；不适用于SVM、线性回归、神经网络等。
	7. 比例特征（ratio feature）：X_1 / X_2
	8. 绝对值（absolute value）
	9. max(X_1, X_2)，min(X_1, X_2)，X_1 xor X_2
	10. 类别特征与数值特征的组合：用N1和N2表示数值特征，用C1和C2表示类别特征，利用pandas的groupby操作，可以创造出以下几种有意义的新特征：（其中，C2还可以是离散化了的N1）

		> median(N1)_by(C1)  \\ 中位数 

		> mean(N1)_by(C1)  \\ 算术平均数

		> mode(N1)_by(C1)  \\ 众数

		> min(N1)_by(C1)  \\ 最小值

		> max(N1)_by(C1)  \\ 最大值

		> std(N1)_by(C1)  \\ 标准差

		> var(N1)_by(C1)  \\ 方差

		> freq(C2)_by(C1)  \\ 频数

		> freq(C1) \\这个不需要groupby也有意义

		仅仅将已有的类别和数值特征进行以上的有效组合，就能够大量增加优秀的可用特征。

		将这种方法和线性组合等基础特征工程方法结合（仅用于决策树），可以得到更多有意义的特征，如：

		> N1 - median(N1)_by(C1)

		> N1 - mean(N1)_by(C1)
	
	11.  用基因编程创造新特征 [Welcome to gplearn’s documentation!](http://gplearn.readthedocs.io/en/stable/index.html)
 
		基于genetic programming的symbolic regression，具体的原理和实现参见文档。目前，python环境下最好用的基因编程库为gplearn。基因编程的两大用法：
		
		* 转换（transformation）：把已有的特征进行组合转换，组合的方式（一元、二元、多元算子）可以由用户自行定义，也可以使用库中自带的函数（如加减乘除、min、max、三角函数、指数、对数）。组合的目的，是创造出和目标y值最“相关”的新特征。这种相关程度可以用spearman或者pearson的相关系数进行测量。spearman多用于决策树（免疫单特征单调变换），pearson多用于线性回归等其他算法。
		* 回归（regression）：原理同上，只不过直接用于回归而已。
	
	12. 用决策树创造新特征
		
		在决策树系列的算法中（单棵决策树、gbdt、随机森林），每一个样本都会被映射到决策树的一片叶子上。因此，我们可以把样本经过每一棵决策树映射后的index（自然数）或one-hot-vector（哑编码得到的稀疏矢量）作为一项新的特征，加入到模型中。 具体实现：apply()以及decision_path()方法，在scikit-learn和xgboost里都可以用。
	
	13. Histogram映射：
	
		把每一列的特征拿出来，根据target内容做统计，把target中的每个内容对应的百分比填到对应的向量的位置。优点是把两个特征联系起来。 

		例如我们来统计“性别与爱好的关系”，性别有“男”“女”，爱好有三种，表示成向量[散步、足球、看电视剧]，分别计算男性和女性中每个爱好的比例得到：男[1/3, 2/3, 0]，女[0, 1/3, 2/3]。即反映了两个特征的关系。
	
	14. 特征交叉
	
		特征交叉对于线性模型可以学习到非线性特征。例如两个特征：年龄和性别，可以组合成 年龄_性别 的一个新特征，比如M_18，F_22等等，然后再对这个特征做one hot编码，即可得到新的特征属性值。至于交叉特征的新特征向量是不是原特征向量的内积，答案是否。因为原特征也是one hot的离散变量，长度不一定相等，如果做向量内积是0。所以正确的做法是，先用原语义做简单字符串拼接，然后再做onehot编码。不过，暴力做交叉特征可能产生的稀疏的问题，这就是另一个问题了。可以参考FM和FFM的解决方案，LIBFFM的库，以及阿里妈妈发布的MLR算法。

	
* 特征升维
	1. 核方法
	2. autoencoder
	3. （CNN）多层神经网络编码

但是，并不是特征构造越多就效果越好。参见[维度灾难](https://zhuanlan.zhihu.com/p/27488363)。分类效果一开始会随着特征数量增加而提升，但到达顶峰后便会一直下降。有两个原因：

1. 特征越多，越容易过拟合；
2. 特征越多，数据越稀疏，需要相应增加的训练数据成指数级增长。

且在高维空间数据都分布在边角，中间几乎没有数据。因此，在高维空间用距离来衡量样本相似性的方法已经渐渐失效。

### 1.0.6 特征选择 
特征选择的的一般流程就是， 找一个集合，然后针对某个学习算法， 测试效果如何， 一直循环直到找到最优集合为止。但时间花费很大。

一般需要考虑两点：

1. 特征是否发散：如果一个特征不发散，就是说这个特征大家都有或者非常相似，说明这个特征不需要。
2. 特征和目标是否相关：与目标的相关性越高，越应该优先选择。

按照特征评价标准分类：

* 选择使分类器的错误概率最小的特征或者特征组合。
* 利用距离来度量样本之间相似度。
* 利用具有最小不确定性（Shannon熵、Renyi熵和条件熵）的那些特征来分类。
* 利用相关系数, 找出特征和类之间存在的相互关系；
* 利用特征之间的依赖关系, 来表示特征的冗余性加以去除。

#### 1.0.7 特征选择 [特征选择， 经典三刀](https://zhuanlan.zhihu.com/p/24635014)

1. 过滤法Filter
	* 方差选择法:计算各个特征方差，选择方差大于阈值的特征 (Analysis of Variance：ANOVA,方差分析，通过分析研究不同来源的变异对总变异的贡献大小，从而确定可控因素对研究结果影响力的大小)。
	* 相关系数法:计算各个特征的Pearson相关系数
			
		皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了 (皮尔逊相关系数，更多反应两个服从正态分布的随机变量的相关性，取值范围在 [-1,+1] 之间。)
	* 互信息法:计算各个特征的信息增益
	* Linear Discriminant Analysis(LDA，线性判别分析)：更     像一种特征抽取方式，基本思想是将高维的特征影到最佳鉴别矢量空间，这样就可以抽取分类信息和达到压缩特征空间维数的效果。投影后的样本在子空间有最大可分离性。
	* Chi-Square：卡方检验，就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合。
		
	优点： 快速， 只需要基础统计知识。缺点：特征之间的组合效应难以挖掘。

2. 封装法Wrapper
	* 递归消除法:使用基模型(如LR)在训练中进行迭代，选择不同
特征
	* 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征
	* 前向选择法：从0开始不断向模型加能最大限度提升模型效果的特征数据用以训练，直到任何训练数据都无法提升模型表现。
	* 后向剃除法：先用所有特征数据进行建模，再逐一丢弃贡献最低的特征来提升模型效果，直到模型效果收敛。

	优点： 直接面向算法优化， 不需要太多知识。缺点： 庞大的搜索空间， 需要定义启发式策略。

3. 嵌入法Embedded(效果最好速度最快，模式单调，快速并且效果明显， 但是如何参数设置， 需要深厚的背景知识。)
	* 使用带惩罚项的基模型进行特征选择
			
		比如LR加入正则。通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验
			
	* 树模型的特征选择(随机森林、决策树)
			
		训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；
	* Lasso
	* Elastic Net
	* Ridge Regression
		
	优点： 快速， 并且面向算法。缺点： 需要调整结构和参数配置， 而这需要深入的知识和经验。

此外还可以通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。

#### 1.0.8 特征降维
特征选择是在原本特征集合中取一部分出来，是特征集合的子集，特征降维做特征的计算组合后构成新特征。 

1. 线性降维
	* 主成分分析(PCA):选择方差最大的K个特征[无监督]
	* 线性判别分析(LDA):选择分类性能最好的特征[有监督]
2. 非线性降维（大多是流行学习）
	* 核主成分分析(KPCA):带核函数的PCA
	* 局部线性嵌入(LLE):利用流形结构进行降维
	* 还有拉普拉斯图、MDS等
3. 迁移成分分析(TCA):不同领域之间迁移学习降维
	* 使用带惩罚项的基模型进行特征选择(比如LR加入正则)
	* 树模型的特征选择(随机森林、决策树)
	
* 人肉：SIFT, VLAD, HOG, GIST, LBP
* 模型：Sparse Coding, Auto Encoders, Restricted Boltzmann Machines, PCA, ICA, K-means

> 工具：Scikit-learn，可以特征选择、降维


#### 1.0.10参考资料
* 知乎live机器学习入门之特征工程——王晋东
* 知乎特征工程话题下部分答案
* [使用sklearn做单机特征工程 ](http://www.cnblogs.com/jasonfreak/p/5448385.html)
* [使用sklearn优雅地进行数据挖掘 ](http://www.cnblogs.com/jasonfreak/p/5448462.html#3955242)
### 1.1 流程及方法
1. 基本的数据挖掘场景
[![2023-11-04-164015.png](https://i.postimg.cc/fbNQrZS9/2023-11-04-164015.png)](https://postimg.cc/bZ35Z7Ty)
2. 特征工程的常见方法和步骤
[2023-11-04-164205.png](https://postimg.cc/rRs1YH4J)
### 1.2 数据描述
通过数据获取，我们得到未经处理的特征，这时的特征可能有以下问题： - 存在缺失值：缺失值需要补充。 - 不属于同一量纲：即特征的规格不一样，不能够放在一起比较。 - 信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。 - 定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。 - 信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的。

那么最好先对数据的整体情况做一个描述、统计、分析，并且可以尝试相关的可视化操作。主要可分为以下几方面：
1. 数据结构
2. 质量检验
   - 标准型、唯一性、有效性、正确性、一致性、异常值、缺失值、重复值
4. 分布情况
   - 统计值：包括max, min, mean, std等。python中用pandas库序列化数据后，可以得到数据的统计值。
   - 探索性数据分析（EDA）
   - 集中趋势、离中趋势、分布形状
### 1.3 特征处理
对数据的整体性有一个宏观的了解之后，即需要进入特征工程第一个重要的环节——特征处理，特征处理会消耗大量时间，并且直接影响特征选择的结果。 特征处理主要包括： ①数据预处理。即数据的清洗工作，主要为缺失值、异常值、错误值、数据格式、采样度等问题的处理。 ②特征转换。即连续变量、离散变量、时间序列等的转换，便于入模。
### 1.4 数据预处理
- 查看整体信息：.info()
- .shape(查看形状)，.size(查看元素个数)
- .head(),tail()分别查看前5行和后5行
- .describe() 查看每一列的统计信息，默认排除所有的NaN元素
- .columns():查看列明
- .issnull.sum(),notnull.sum():查看缺失值总数
- 使用missingno库
  
          import missingno as msno 
           missingValueColumns = merged.columns[merged.isnull().any()].tolist() 
  - 1. bar图缺失值可视化分析
    2. matrix密集图查看数据完整性
    3. heatmap图查看相关性
    4. dendrogram树状图查看相关性
- 使用pandas_profiling库
  
1） 缺失值处理

有些特征可能因为无法采样或者没有观测值而缺失.例如距离特征，用户可能禁止获取地理位置或者获取地理位置失败，此时需要对这些特征做特殊的处理，赋予一个缺失值。我们在进行模型训练时，不可避免的会遇到类似缺失值的情况，下面整理了几种填充空值的方法

- 删除缺失值
  - 删除实例
  - 删除特征
- 缺失值填充
  - 用固定值填充
    - 对于特征值缺失的一种常见的方法就是可以用固定值来填充，例如0，9999， -9999, 例如下面对灰度分这个特征缺失值全部填充为-99

            data['灰度分'] = data['灰度分'].fillna('-99')
      
  - 用均值填充
    - 对于数值型的特征，其缺失值也可以用未缺失数据的均值填充，下面对灰度分这个特征缺失值进行均值填充
   
            data['灰度分']=data['灰度分'].fillna(data['灰度分'].mean())
  - 用众数填充

          data['灰度分']=data['灰度分'].fillna(data['灰度分'].mode())
  - 用上下数据进行填充
    - 用前一个数据进行填充

          data['灰度分']=data['灰度分'].fillna(method='pad')
    - 用后一个数据进行填充

          data['灰度分']=data['灰度分'].fillna(method='bfill')
      
  - 用插值法进行填充

           data['灰度分']=data['灰度分'].interplolate()
  
  - 用KNN进行填充

          from fancyimpute import BiScaler, KNN, NuclearNormMinimization, SoftImpute
          dataset = KNN(k=3).complete(dataset)
  - random forest进行填充
    
          from sklearn.ensemble import RandomForestRegressor
          #下面是出现空值的列
          zero_columns_2 = ['机构查询数量', '直接联系人数量', '直接联系人在黑名单数量', '间接联系人在黑名单数量',
                          '引起黑名单的直接联系人数量', '引起黑名单的直接联系人占比']
          #将出现空值的除了预测的列全部取出来，不用于训练 
          dataset_list2=[x for x in dataset if x not in zero_columns_2]
          dataset_2=dataset[dataset_list2]
          #取出灰度分不是空的全部样本进行训练
          know=dataset_2[dataset_2['灰度分'].notnull()]
          print(know.shape) #26417, 54
          #取出灰度分为空的样本用于填充值
          unknow = dataset_2[dataset_2['灰度分'].isnull()]
          print(unknow.shape) #2078, 54
          y = ['灰度分']
          x = [1]
          know_x2 = know.copy()
          know_y2 = know.copy()
          print(know_y2.shape)
          #将
          know_x2.drop(know_x2.columns[x], axis=1, inplace=True)
          print(know_y2.shape)
          print(know_x2.shape)
          #
          know_y2 = know[y]
          # RandomForestRegressor
          rfr = RandomForestRegressor(random_state=666, n_estimators=2000, n_jobs=-1)
          rfr.fit(know_x2, know_y2)
  - 使用fancyimpute包中的其他方法
          
          # matrix completion using convex optimization to find low-rank solution
          # that still matches observed values. Slow!
          X_filled_nnm = NuclearNormMinimization().complete(X_incomplete)
          # Instead of solving the nuclear norm objective directly, instead
          # induce sparsity using singular value thresholding
          X_filled_softimpute = SoftImpute().complete(X_incomplete_normalized)
          # print mean squared error for the three imputation methods above
          nnm_mse = ((X_filled_nnm[missing_mask] - X[missing_mask]) ** 2).mean()
          # print mean squared error for the three imputation methods above
          nnm_mse = ((X_filled_nnm[missing_mask] - X[missing_mask]) ** 2).mean()
          print("Nuclear norm minimization MSE: %f" % nnm_mse)
          softImpute_mse = ((X_filled_softimpute[missing_mask] - X[missing_mask]) ** 2).mean()
          print("SoftImpute MSE: %f" % softImpute_mse)
          knn_mse = ((X_filled_knn[missing_mask] - X[missing_mask]) ** 2).mean()
          print("knnImpute MSE: %f" % knn_mse)
  - 缺失值作为数据的一部分不填充
    -  LightGBM和XGBoost都能将NaN作为数据的一部分进行学习，所以不需要处理缺失值。
2） 异常值处理
- 特征平滑处理
  - 基于统计的异常点检测算法 例如极差，四分位数间距，均差，标准差等，这种方法适合于挖掘单变量的数值型数据。
  - 基于距离的异常点检测算法 主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，主要使用的距离度量方法有绝对距离(曼哈顿距离)、欧氏距离和马氏距离等方法。
  - 基于密度的异常点检测算法 考察当前点周围密度，可以发现局部异常点
- 重复值处理
  - 根据需求判断是否需要去重操作
- 数据格式出
  - 数字类型的转换
  - 数字单位的调整
  - 时间格式的处理
- 数据采样
  - 多的类别过采样/少的类别欠采样来平衡分布欠采样（undersampling）和过采样（oversampling）会对模型带来不一样的影响。
# 填充为空的样本

          unknow_x2 = unknow.copy()
          unknow_x2.drop(unknow_x2.columns[x], axis=1, inplace=True)
          print(unknow_x2.shape) #(2078, 53)
          unknow_y2 = rfr.predict(unknow_x2)
          unknow_y2 = pd.DataFrame(unknow_y2, columns=['灰度分'])
### 1.5 特征转换
特征也就是我们常常说的变量/自变量，一般分为三类：
- 连续型
- 无序类别（离散）型
- 有序类别（离散）型

- 主要的转换方式有以下几种

  [![2023-11-04-174152.png](https://i.postimg.cc/43rm4GQP/2023-11-04-174152.png)](https://postimg.cc/McmWth4f)
1. 连续型特征处理
   - 函数转换
     - 有时我们的模型的假设条件是要求自变量或因变量服从某特殊分布（如正太分布），或者说自变量或因变量服从该分布时，模型的表现较好。这个时候我们就需要对特征或因变量进行非线性函数转换。这个方法操作起来很简单，但记得对新加入的特征做归一化。对于特征的转换，需要将转换之后的特征和原特征一起加入训练模型进行训练。
   - 特征缩放
     - 某些特征比其他特征具有较大的跨度值。举个例子，将一个人的收入和他的年龄进行比较，更具体的例子，如某些模型（像岭回归）要求你必须将特征值缩放到相同的范围值内。通过缩放可以避免某些特征比其他特征获得大小非常悬殊的权重值。
   - 无量纲化
     - 无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化、归一化、区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。 把数据放缩到同样的范围 SVM/NN影响很大 树模型影响小。不是什么时候都需要标准化，比如物理意义非常明确的经纬度，如果标准化，其本身的意义就会丢失。
     - 标准化：均值方差法，z-score标准化，standardScaler标准化
     - 使用preproccessing库的StandardScaler类对数据进行标准化的代码如下：

          from sklearn.preprocessing import StandardScaler
          #标准化，返回值为标准化后的数据
          StandardScaler().fit_transform(iris.data)
     - L2归一化的
[![2023-11-04-181117.png](https://i.postimg.cc/9FmFhb7r/2023-11-04-181117.png)](https://postimg.cc/PN7kbmKj)
     - 区间缩放法
[![2023-11-04-181117.png](https://i.postimg.cc/9FmFhb7r/2023-11-04-181117.png)](https://postimg.cc/PN7kbmKj)
     - 二值化（定量特征）
  [![2023-11-04-181327.png](https://i.postimg.cc/VNpWy5Zk/2023-11-04-181327.png)](https://postimg.cc/GBPyG31W)
     - 离散化分箱处理（数值型转类别型）
       - 有时候，将数值型属性转换成类别型更有意义，同时将一定范围内的数值划分成确定的块，使算法减少噪声的干扰。 在实际应用中，当你不想让你的模型总是尝试区分值之间是否太近时，分区能够避免过拟合。例如，如果你所感兴趣的是将一个城市作为整体，这时你可以将所有落入该城市的维度值进行整合成一个整体。 分箱也能减小错误的影响，通过将一个给定值划入到最近的块中。 对于一些特殊的模型（信用评分卡）开发，有时候我们需要对连续型的特征（年龄、收入）进行离散化。 常用的离散化方法包括等值划分和等量划分。例如某个特征的取值范围为[0，10]，我们可以将其划分为10段，[0,1),[1,2),⋯,[9,10)
         - 离散特征的增加和减少都很容易，易于模型的快速迭代；
         - 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
         - 离散化后的特征对异常数据有很强的鲁棒性模型也会更稳定；
         - 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性 提 升表达能力；
         - 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险
       - 离散化方法的关键是怎么确定分段中的离散点，下面介绍几种常用的离散化方法：
         - 等距离离散（等距分组） 顾名思义，就是离散点选取等距点。
         - 等样本点离散（等深分组） 选取的离散点保证落在每段里的样本点数量大致相同
         - 决策树离散化（最优分组） 决策树离散化方法通常也是每次离散化一个连续特征，原理如下： 单独用此特征和目标值y训练一个决策树模型，然后把训练获得的模型内的特征分割点作为离散化的离散点。
         - 其他离散化方法 其中，最优分组除决策树方法以外，还可以使用卡方分箱的方法，这种方法在评分卡开发中比较常见。
    - 不处理
      - 除了归一化（去中心，方差归一），不用做太多特殊处理，可以直接把连续特征扔到模型里使用。根据模型类型而定。
  
2. 离散型特征处理
   - 数值化处理
     - 二分类问题：能够将类别属性转换成一个标量，最有效的场景应该就是二分类的情况。即{0,1}对应{类别1，类别2}。这种情况下，并不需要排序，并且你可以将属性的值理解成属于类别1或类别2的概率。 多分类问题：选取多分类，编码到[0，classnum)。 类别不平衡问题：样本层面可以采用oversampling/undersampling. 算法层面可以采用代价敏感方法/样本设置权重 也不是所有的无序变量都需要做数值化处理，决策树、随机森林等树模型可能不需要处理，视情况而定。
     - 例：label encoder 一个变量的k个值，按序转换成k个数字（1，2，3…k）。例如一个人的状态status有三种取值：bad, normal, good，显然bad < normal < good。这个时候bad, normal, good就可以分别转换成 1、2、3。该方法局限性较大：
       - 不适用于建立预测具体数值的模型，比如线性回归，只能用于分类，
       - 即使用于分类，也有一些模型不适合， - 可能结果的精度不如one-hot编码。
   - 哑编码
     - 独热编码
       - 优点： 简单，且保证无共线性。 将离散型特征使用one-hot编码，确实会让特征之间的距离计算更加合理。 对离散型特征进行one-hot编码可以加快计算速度。
       - 缺点：太稀（稀疏矩阵） 避免产生稀疏矩阵的常见方法是降维，将变量值较多的分类维度，尽可能降到最少，能降则降，不能降的，别勉强。
     - 顺序哑变量
       - 与one-hot编码一样，都是将一个变量的k个值生成k个哑变量，但同时保护了特征的顺序关系。一般的表达方式如下：
[![2023-11-04-194602.png](https://i.postimg.cc/XYY7ST8D/2023-11-04-194602.png)](https://postimg.cc/YhTwQDT1)
   - 时间序列处理
     - 时间戳属性通常需要分离成多个维度比如年、月、日、小时、分钟、秒钟。但是在很多的应用中，大量的信息是不需要的。比如在一个监督系统中，尝试利用一个’位置+时间‘的函数预测一个城市的交通故障程度，这个实例中，大部分会受到误导只通过不同的秒数去学习趋势，其实是不合理的。并且维度’年’也不能很好的给模型增加值的变化，我们可能仅仅需要小时、日、月等维度。因此在呈现时间的时候，试着保证你所提供的所有数据是你的模型所需要的。并且别忘了时区，假如你的数据源来自不同的地理数据源，别忘了利用时区将数据标准化。
### 1.6 特征选择
1. 定义
   - 从大量的特征中选择少量的有用特征。 不是所有的特征都是平等的。那些与问题不相关的属性需要被删除；还有一些特征可以比其他特征更重要；也有的特征跟其他的特征是冗余的。特征选择就是自动地选择对于问题最重要的特征的一个子集。
2. 作用
   - 简化模型，增加模型的可解释性
   - 缩短训练时间
   - 避免维度灾难
   - 改善模型通用性、降低过拟合
4. 方法
- 判断特征是否发散：如果一个特征不发散，就是说这个特征大家都有或者非常相似，说明这个特征不需要。
- 判断特征和目标是否相关：与目标的相关性越高，越应该优先选择。
- 按照特征评价标准分类： 选择使分类器的错误概率最小的特征或者特征组合。
- 利用距离来度量样本之间相似度。
- 利用具有最小不确定性（Shannon熵、Renyi熵和条件熵）的那些特征来分类。
- 利用相关系数, 找出特征和类之间存在的相互关系；
- 利用特征之间的依赖关系, 来表示特征的冗余性加以去除。
-  特征选择算法可以利用得分排序的方法选择特征，如相关性和其他特征重要性手段；
- 更高级的方法通过试错来搜索特征子集。这些方法通过建立模型，评价模型，然后自动的获得对于目标最具预测能力的特征子集。
- 还有一些算法能得到特征选择的副产品。比如说逐步回归就是能够自动的选择特征来构建模型。
- 正则化的方法比如lasso和岭回归可以作为特征选择的算法。他们在构建模型的过程中删去或者减小不重要特征的贡献。（An Introduction to feature selection）

根据特征选择的形式可以将选择特征的方法分为3种

- Fillter:过滤法。判断特征是否发散：如果一个特征不发散，就是说这个特征大家都有或者非常相似，说明这个特征不需要。 判断特征和目标是否相关：与目标的相关性越高，越应该优先选择。 按照特征评价标准分类： 选择使分类器的错误概率最小的特征或者特征组合。 利用距离来度量样本之间相似度。 利用具有最小不确定性（Shannon熵、Renyi熵和条件熵）的那些特征来分类。 利用相关系数, 找出特征和类之间存在的相互关系； 利用特征之间的依赖关系, 来表示特征的冗余性加以去除。 - 特征选择算法可以利用得分排序的方法选择特征，如相关性和其他特征重要性手段； - 更高级的方法通过试错来搜索特征子集。这些方法通过建立模型，评价模型，然后自动的获得对于目标最具预测能力的特征子集。 - 还有一些算法能得到特征选择的副产品。比如说逐步回归就是能够自动的选择特征来构建模型。 - 正则化的方法比如lasso和岭回归可以作为特征选择的算法。他们在构建模型的过程中删去或者减小不重要特征的贡献。（An Introduction to feature selection）
- Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
- Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。
[![2023-11-04-200129.png](https://i.postimg.cc/hjMW4GpF/2023-11-04-200129.png)](https://postimg.cc/7GCQmwFX)
1) 过滤式

过滤式特征选择的评价标准从数据集本身的内在性质获得，与特定的学习算法无关，因此具有较好的通用性。通常选择和类别相关度大的特征或者特征子集。过滤式特征选择的研究者认为，相关度较大的特征或者特征子集会在分类器上获得较高的准确率。过滤式特征选择的评价标准分为四种，即距离度量、信息度量、关联度度量以及一致性度量。

- 优点：算法的通用性强；省去了分类器的训练步骤，算法复杂性低，因而适用于大规模数据集；可以快速去除大量不相关的特征，作为特征的预筛选器非常合适。
- 缺点：由于算法的评价标准独立于特定的学习算法，所选的特征子集在分类准确率方面通常低于Wrapper方法。
>- 方差选择法
  - 使用方差选择法，先要计算各个特征的方差，选择方差大于阈值的特征 (Analysis of Variance：ANOVA,方差分析，通过分析研究不同来源的变异对总变异的贡献大小，从而确定可控因素对研究结果影响力的大小)。

          from sklearn.feature_selection import VarianceThreshold
          #方差选择法，返回值为特征选择后的数据
          #参数threshold为方差的阈值
          VarianceThreshold(threshold=3).fit_transform(iris.data)
>- 相关系数法
  - 使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。用feature_selection库的SelectKBest类结合相关系数来选择特征的代码如下：

          from sklearn.feature_selection import SelectKBest
          from scipy.stats import pearsonr
          #选择K个最好的特征，返回选择特征后的数据
          #第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数
          #参数k为选择的特征个数
          SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
>- 互信息方法
  - 计算各个特征的信息增益 Linear Discriminant Analysis(LDA，线性判别分析)：更 像一种特征抽取方式，基本思想是将高维的特征影到最佳鉴别矢量空间，这样就可以抽取分类信息和达到压缩特征空间维数的效果。投影后的样本在子空间有最大可分离性。 经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：
[![2023-11-04-201751.png](https://i.postimg.cc/HLb1RgKM/2023-11-04-201751.png)](https://postimg.cc/qtBF69pB)
  - 为了处理定量数据，最大信息系数法被提出，使用feature_selection库的SelectKBest类结合最大信息系数法来选择特征的代码如下：

          from sklearn.feature_selection import SelectKBest
          from minepy import MINE
          #由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5
          def mic(x, y):
              m = MINE()
              m.compute_score(x, y)
              return (m.mic(), 0.5)
          #选择K个最好的特征，返回特征选择后的数据
          SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
>- 卡方检验法
  - 就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合。 优点： 快速， 只需要基础统计知识。
  - 缺点：特征之间的组合效应难以挖掘。 经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：
  - 这个统计量的含义简而言之就是自变量对因变量的相关性。用feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下：
[![2023-11-04-202257.png](https://i.postimg.cc/3NYLNMnP/2023-11-04-202257.png)](https://postimg.cc/XG1K1PM8)

          from sklearn.feature_selection import SelectKBest
          from sklearn.feature_selection import chi2 
          #选择K个最好的特征，返回选择特征后的数据
          SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)
    - 特点： 执行时间短（+）：一般不会在数据集上进行迭代计算，且是分类器无关的，相比于训练分类器要快。 一般性（+）：评价数据中属性的本身的性质，而不是属性与特定分类器的关联关系（适合程度），所以得到的结果更具有一般性，而且对于很多分类器都能表现“良好”。 选择大规模子集（-）：倾向于选择全部最优的特征子集，导致决定停止的条件决定权交给了用户，具有较强的主观性
2）封装

Wrapper：封装式特征选择是利用学习算法的性能评价特征子集的优劣。因此，对于一个待评价的特征子集，Wrapper方法需要训练一个分类器，根据分类器的性能对该特征子集进行评价。Wrapper方法中用以评价特征的学习算法是多种多样的，例如决策树、神经网络、贝叶斯分类器、近邻法、支持向量机等等。 
- 优点：相对于Filter方法，Wrapper方法找到的特征子集分类性能通常更好。
- 缺点：Wrapper方法选出的特征通用性不强，当改变学习算法时，需要针对该学习算法重新进行特征选择；由于每次对子集的评价都要进行分类器的训练和测试，所以算法计算复杂度很高，尤其对于大规模数据集来说，算法的执行时间很长。
[![2023-11-04-204830.png](https://i.postimg.cc/mkqqXpbk/2023-11-04-204830.png)](https://postimg.cc/0KYcrfK1)
- 完全搜索
  - 递归特征消除法
    - 递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用feature_selection库的RFE类来选择特征的代码如下：
  - 递归特征消除法递归消除法
    - 使用基模型(如LR)在训练中进行迭代，选择不同 特征 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征
- 启发搜索法
  - 前向选择法
    - 从0开始不断向模型加能最大限度提升模型效果的特征数据用以训练，直到任何训练数据都无法提升模型表现。
  - 后向剔除法
    - 先用所有特征数据进行建模，再逐一丢弃贡献最低的特征来提升模型效果，直到模型效果收敛。 优点： 直接面向算法优化， 不需要太多知识。缺点： 庞大的搜索空间， 需要定义启发式策略。

          from sklearn.feature_selection import RFE
          from sklearn.linear_model import LogisticRegression
          #递归特征消除法，返回特征选择后的数据
          #参数estimator为基模型
          #参数n_features_to_select为选择的特征个数
          RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)
- 随机搜索法（策略+运气）

Wrapper特点：
- 准确性（+）：由于特征子集是针对特定的分类器调准，能获得较好的识别率。
- 泛化能力（+）：有很多机制可以防止过拟合，例如在分类器中可以使用交叉验证等技术。
- 执行速度（-）：对于每个候选的特征子集，必须重新训练一个分类器甚至多个分类器（交叉验证），所以对于计算密集型非常不适合。
- 一般性（-）：由于此种方法对分类器敏感，而不同的分类器具有不同原理的评价函数（损失函数），只能说最终选择特征子集是对当前分类器“最好”的。
3) 嵌入法

- 在嵌入式特征选择中，特征选择算法本身作为组成部分嵌入到学习算法里。最典型的即决策树算法，如ID3、C4.5以及CART算法等，决策树算法在树增长过程的每个递归步都必须选择一个特征，将样本集划分成较小的子集，选择特征的依据通常是划分后子节点的纯度，划分后子节点越纯，则说明划分效果越好，可见决策树生成的过程也就是特征选择的过程。
- 嵌入法Embedded(效果最好速度最快，模式单调，快速并且效果明显， 但是如何参数设置， 需要深厚的背景知识。) 在模型既定的条件下，寻找最优特征子集 - 正则化项（ L1、L2 ） - LASSO回归 - 岭回归（RidgeRegression） - 决策树 - ID3、C4.5、CART - 深度学习
>- 基于惩罚项
 > >- 使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。 使用带惩罚项的基模型进行特征选择 比如LR加入正则。通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验 使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下：

          from sklearn.feature_selection import SelectFromModel
          from sklearn.linear_model import LogisticRegression
          #带L1惩罚项的逻辑回归作为基模型的特征选择
          SelectFromModel(LogisticRegression(penalty="l1", C=0.1)).fit_transform(iris.data, iris.target)
  >>- L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1，选择在L2中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型：

          from sklearn.linear_model import LogisticRegression
          
          class LR(LogisticRegression):
              def __init__(self, threshold=0.01, dual=False, tol=1e-4, C=1.0,
                           fit_intercept=True, intercept_scaling=1, class_weight=None,
                           random_state=None, solver='liblinear', max_iter=100,
                           multi_class='ovr', verbose=0, warm_start=False, n_jobs=1):
          
                  #权值相近的阈值
                  self.threshold = threshold
                  LogisticRegression.__init__(self, penalty='l1', dual=dual, tol=tol, C=C,
                           fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight,
                           random_state=random_state, solver=solver, max_iter=max_iter,
                           multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)
                  #使用同样的参数创建L2逻辑回归
                  self.l2 = LogisticRegression(penalty='l2', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight = class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)
          
              def fit(self, X, y, sample_weight=None):
                  #训练L1逻辑回归
                  super(LR, self).fit(X, y, sample_weight=sample_weight)
                  self.coef_old_ = self.coef_.copy()
                  #训练L2逻辑回归
                  self.l2.fit(X, y, sample_weight=sample_weight)
          
                  cntOfRow, cntOfCol = self.coef_.shape
                  #权值系数矩阵的行数对应目标值的种类数目
                  for i in range(cntOfRow):
                      for j in range(cntOfCol):
                          coef = self.coef_[i][j]
                          #L1逻辑回归的权值系数不为0
                          if coef != 0:
                              idx = [j]
                              #对应在L2逻辑回归中的权值系数
                              coef1 = self.l2.coef_[i][j]
                              for k in range(cntOfCol):
                                  coef2 = self.l2.coef_[i][k]
                                  #在L2逻辑回归中，权值系数之差小于设定的阈值，且在L1中对应的权值为0
                                  if abs(coef1-coef2) < self.threshold and j != k and self.coef_[i][k] == 0:
                                      idx.append(k)
                              #计算这一类特征的权值系数均值
                              mean = coef / len(idx)
                              self.coef_[i][idx] = mean
                  return self
  - 使用feature_selection库的SelectFromModel类结合带L1以及L2惩罚项的逻辑回归模型，来选择特征的代码如下：

          from sklearn.feature_selection import SelectFromModel
          #带L1和L2惩罚项的逻辑回归作为基模型的特征选择
          #参数threshold为权值系数之差的阈值
          SelectFromModel(LR(threshold=0.5, C=0.1)).fit_transform(iris.data, iris.target)

>- 基于树模型
  >>- 基于树模型的特征选择法 树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型，来选择特征的代码如下：训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型； - Lasso - Elastic Net - Ridge Regression 优点： 快速， 并且面向算法。缺点： 需要调整结构和参数配置， 而这需要深入的知识和经验。

          from sklearn.feature_selection import SelectFromModel
          from sklearn.ensemble import GradientBoostingClassifier
          #GBDT作为基模型的特征选择
          SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)
>- 深度学习
  >>- 目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。 在特征学习中，K-means算法可以将一些没有标签的输入数据进行聚类，然后使用每个类别的质心来生成新的特征。

## 2. 特征构造
- 定义
  - 从原始数据中构造新特征，在机器学习或者统计学中，又称为变量选择、属性选择或者变量自己选择，实在模型构件中，选择相关特征并构成特征子集的过程。根据已有特征生成新特征，增加特征的非线性。常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。特征工程中引入新特征，需要验证它确实能够提高预测的准确度，而不是加入一个无用的特征增加算法运算的复杂度。特征重要性和特征选择可以告诉你特征的效用。你需要构造新的特征出来。这就要求你在样本数据上花费大量的实践并思考问题的本质，数据集结构，以及怎么最好的在预测模型中利用他们。
- 作用
  - 对于表格型数据，通常对特征进行混合聚集，组合或者分解分割来创造新的特征；
  - 对于文本型数据通常需要设计特定的与问题相关的文档指标；
  - 对于图像数据通常需要花费大量时间指示过滤器发现相关的特征。
  - 这部分就是人们通常认为的特征工程最具有艺术性的部分。这是一个漫长的部分，需要耗费大量的脑力和实践，并可以产生巨大的作用。

### 2.1 特征构造的方法
1. 简单构造
   - 四则运算
     - 比如原来的特征是x1和x2，那么x1+x2就是一个新的特征，或者当x1大于某个数c的时候，就产生一个新的变量x3，并且x3=1，当x1小于c的时候，x3=0，所以这样看来呢，可以按照这种方法构造出很多特征，这个就是构造。
   - 特征交叉
     - 交叉特征算是特征工程中非常重要的方法之一了，它是将两个或更多的类别属性组合成一个。当组合的特征要比单个特征更好时，这是一项非常有用的技术。数学上来说，是对类别特征的所有可能值进行交叉相乘。假如拥有一个特征A,A有两个可能值{A1，A2}。拥有一个特征B，存在{B1，B2}等可能值。然后，A&B之间的交叉特征如下：{（A1，B1），（A1，B2），（A2，B1），（A2，B2）}，并且你可以给这些组合特征取任何名字，但是需要明白每个组合特征其实代表着A和B各自信息协同作用。一个更好地诠释好的交叉特征的实例是类似于（经度，纬度）。一个相同的经度对应了地图上很多的地方，纬度也是一样。但是一旦你将经度和纬度组合到一起，它们就代表了地理上特定的一块区域，区域中每一部分是拥有着类似的特性。
   - 分解类别特征
     - 对于一个特征item_color有‘red’、‘green’、‘unknown’三个取值，那么可以创造一些新的特征例如：
       - 二值特征has_color： 1知道具体颜色，0表示不知道。这个可以替换item_color特征用到更简单的线性模型中。
       - 三个二值特征is_red、is_green和is_unknown。这个可以添加到原有特征中用到决策树模型中。
   - 重构数值量
     - 单位转换 整数部分与小数部分分离 构造范围二值特征 构造阶段性的统计特征 构造其他二值特征
   - 分解Datatime
   - 窗口变量统计
3. 机器学习
   - 监督学习
   - 非监督学习
  
#### 2.1.1 构造特征_交叉特征
https://zhuanlan.zhihu.com/p/457853657

特征交叉分类
- 显示交叉：可以从暴力解法、数学方式来理解
- 隐式交叉，可以人为是为了解决显示交叉的缺点提出的，主要是和神经网络结合

1. 显示交叉
[![2023-11-05-092813.png](https://i.postimg.cc/fRjmjSHG/2023-11-05-092813.png)](https://postimg.cc/v4T4GTwh)
- 局限性
  - 对非线性建模能力是有限的
  - 很难扩展到更高阶的特征交叉
  - 但数据稀疏性很大时，模型的训练比较困难
  - 对所有特征交叉一视同仁，可能会限制模型的表达能力
  - 不能自动化实现特征交叉
  
因此，为了解决上述的问题，提高模型的表达能力，相关研究人员给出了不同的解决方案。加上深度学习时代的到来，特征交叉方案变得更加丰富，缓解上述的挑战。常见的特征交叉方式有MLP网络、Product网络、NFM（Bi-Interaction Pooling层 + MLP层）、Cross网络、CIN网络、CAN网络等。在这里，认为这些特征交叉为隐式交叉。
#### 2.1.2 特征构造——特征叫交叉——显示交叉（FM模型）
https://mp.weixin.qq.com/s?__biz=Mzg5NTYyMDgyNg==&mid=2247489278&idx=1&sn=f3652394955d719bf02a91ca3b179ed2&source=41#wechat_redirect
>- 浅层模型发展史

LR->poky2->FM->FFM->FwFM

**1. Logistic Regression (LR)**

  - 使用线性权重组合每个单一的特征
  - 优点：LR模型简单，计算快。
  - 缺点：LR不具备特征交互的能力，交叉特征可能比单一特征具有更重要的影响。

**2. Poly2模型**
    
[![2023-11-05-104700.png](https://i.postimg.cc/K8dNNpMR/2023-11-05-104700.png)](https://postimg.cc/87mMkHwS)
   - 优点：Poly2可以捕获特征交互。
   - 缺点：Poly2的参数量为O(m*m)，在CTR这样的稀疏场景下仅有少量的特征对会同时出现，因此Poly2的权重稀疏无法得到有效的学习。
     
**3. FM模型**
  
FM模型的提出，是为了解决当数据稀疏时，如何更好地训练模型。在polo2模型中，每一项二阶交叉特征交叉项前面都赋予一定权重让模型自己学习，但是，推荐数据往往是稀疏的，导致很多权重并不能被训练充分。音系，让FM模型结合了矩阵分解的思想，对polo2模型进行了改进

[![2023-11-05-093801.png](https://i.postimg.cc/JhCMNK6q/2023-11-05-093801.png)](https://postimg.cc/SndBqLp2)

- 说到推荐、广告的算法模型，几乎很难绕开FM，它是一个非常强的模型。理论简单、推导严谨、实现容易，并且效果不俗。即使是目前仍然在各大厂商当中发挥用场，在一些小厂当中甚至是主力模型。我们初看之前也许还有疑惑，但是相信当大家看完之后，必然会有全新的认识。
- FM的全称是Factorization Machines，翻译过来就是因式分解机
- 优点
  - 与SVM对比，能够在系数的特征当中仍然拥有很好的表现
  - 效率高，可以在线性时间内获得结果
  - 不像非线性SVM（和函数），FM并不需要转换成对偶形式，并且模型的参数可以直接训练，也不用借助支持向量或者是其他方法
  - 除了SVM之外，FM模型和其他的因式分解模型比如SVD、PITF、FPMC比较起来都有非常明显的优势。这些模型往往只针对一个特定的场景或者是特定的数据集，并且它们的训练以及优化方案都是根据场景定制化的。FM模型不需要定制化就可以实现同样好的效果，可以非常简易地应用在各个预测问题当中。
  
优点的总结
- FM模型的参数支持非常稀疏的特征，而SVM等模型不行
- FM的时间复杂度为，并且可以直接优化原问题的参数，而不需要依靠支持向量或者是转化成对偶问题解决
- FM是通用的模型，可以适用于任何实数特征的场景，其他的模型不行

>- 稀疏场景

在有监督学习的场景当中，最常见的任务就是给定一个向量x，要求预测一个目标T。如果T是一个实数，那么这就是回归模型，如果T是一个类别的常量，就是一个分类模型。

这些都是机器学习的基础知识，相信大家都了解，然而对于线上排序的功能来说，我们重要的是给商品排序，而不是分类。常规来说我们可以将impression和click看成是两个类别进行分类预测，也可以直接用回归模型预测商品的点击率，根据点击率进行排序。这两种其实本质上是一样的，都是针对商品本身进行学习。还有一种方法是更加侧重商品之间的排序，我们的模型学习的是商品之间的优劣关系。

举个例子，比如我们用向量xi表示i商品的特征向量，xj表示j商品的特征向量，那么我们可以将这两个向量合并一起作为输入，进行分类预测。分类的结果表示的是i商品在j商品之前还是反之。这样我们可以通过多次预测，直接得到商品之间的排序关系，而不是根据一个分数进行排序。这样可以在只有正样本的情况下进行训练。这种方法直接训练的模型商品的优劣，业内叫做**learning to rank**。

然而不论是哪一种做法，都有一个问题绕不开就是特征的稀疏。举个很简单的例子，比如我们对商品的类目进行one-hot处理，在电商场景下商品的类目动辄上万个，那么one-hot之后的结果就是一个长度上万的数组，并且这个数组当中只有一位为1，其他均为0。当然这只是一个例子，除此之外还有许多许多的特征有可能是非常稀疏的。

>- FM模型原理
[![2023-11-05-101817.png](https://i.postimg.cc/hthDrf1C/2023-11-05-101817.png)](https://postimg.cc/v4Fw88j9)
[![2023-11-05-101848.png](https://i.postimg.cc/6qYKVX2J/2023-11-05-101848.png)](https://postimg.cc/ZvBXZXxV)
这种做法有两种理解方式，一种就是刚才说的，我们对于一些稀疏的组合也可以很好地计算出系数。另外一种理解方式是这其实也是一种embedding的方式，将某一个id映射成向量。所以业内也有使用FM来做embedding的。
>- 复杂度优化
[![2023-11-05-102315.png](https://i.postimg.cc/bN5cqL1T/2023-11-05-102315.png)](https://postimg.cc/3d2bBXS4)
[![2023-11-05-102345.png](https://i.postimg.cc/QthvfzrX/2023-11-05-102345.png)](https://postimg.cc/67bH3Y3S)
>- 模型的训练
[![2023-11-05-102608.png](https://i.postimg.cc/7Zzcw4HH/2023-11-05-102608.png)](https://postimg.cc/dhqnjzZp)
>- 扩展到d维
[![2023-11-05-102651.png](https://i.postimg.cc/6QNPQPqd/2023-11-05-102651.png)](https://postimg.cc/QF0m40pt)
>- 代码实现
- pytorch环境下的实现

               import torch
               from torch import nn
               
               ndim = len(feature_names)
               k = 4
               
               class FM(nn.Module):
                   def __init__(self, dim, k):
                       super(FM, self).__init__()
                       self.dim = dim
                       self.k = k
                       self.w = nn.Linear(self.dim, 1, bias=True)
                       # 初始化V矩阵
                       self.v = nn.Parameter(torch.rand(self.dim, self.k) / 100)
                       
                   def forward(self, x):
                       linear = self.w(x)
                       # 二次项
                       quadradic = 0.5 * torch.sum(torch.pow(torch.mm(x, self.v), 2) - torch.mm(torch.pow(x, 2), torch.pow(self.v, 2)))
                       # 套一层sigmoid转成分类模型，也可以不加，就是回归模型
                       return torch.sigmoid(linear + quadradic)
                   
                   
               fm = FM(ndim, k)
               loss_fn = nn.BCELoss()
               optimizer = torch.optim.SGD(fm.parameters(), lr=0.005, weight_decay=0.001)
               iteration = 0
               epochs = 10
               
               for epoch in range(epochs):
                   fm.train()
                   for X, y in data_iter:
                       output = fm(X)
                       l = loss_fn(output.squeeze(dim=1), y)
                       optimizer.zero_grad()
                       l.backward()
                       optimizer.step()
                       iteration += 1        
                       if iteration % 200 == 199:
                           with torch.no_grad():
                               fm.eval()
                               output = fm(X_eva_tensor)
                               l = loss_fn(output.squeeze(dim=1), y_eva_tensor)
                               acc = ((torch.round(output).long() == y_eva_tensor.view(-1, 1).long()).sum().float().item()) / 1024
                               print('Epoch: {}, iteration: {}, loss: {}, acc: {}'.format(epoch, iteration, l.item(), acc))
                           fm.train()
- 优点：FM可以捕获特征交互，同时可以在稀疏场景下有效的学习
- 缺点：当一个特征与其他域的特征进行交互时，它的行为可能有所不同（比如，男性和化妆品做交叉特征，男性和星期一做交叉特征，此时用到的男性的特征向量都是一样的，但是这可能有问题，化妆品和星期一都属于不同事物，那么男性这一特征面对不同事物做特征交叉时，是不是应该有所不同呢？），所以就有了FFM模型

**4. FFM模型**

- 为每个特征学习n-1(n为field的个数)个向量表示，与来自不同域的特征交互时使用不同的向量表示。
![image](https://github.com/123456789bhf/zhang/assets/116550706/7a51a6cb-723c-44f6-80d2-c381865a99a8)

- 优点：FFM可以捕获特征交互，考虑了field信息
- 缺点：参数量为O(m+n*m*k),在实际的生产系统中，FFM中大量参数是不可接受的
  
**5. FwFM**

https://mp.weixin.qq.com/s/6x2VKkAlRBEm5xFVCYInEg

- 显式地建模了不同的Field相互交互的强度。
[![2023-11-05-112530.png](https://i.postimg.cc/TY5QDXKZ/2023-11-05-112530.png)](https://postimg.cc/dLvGbpmB)
- 优点：FFM可以捕获特征交互，获得了field信息，相比FM仅仅增加了n*(n-1)/2个需要学习的下参数（n一般仅为几十或者几百），仅用FFM4%左右的参数便可达到相媲美的效果。FwFM已经被部署到很多大厂的广告系统中。
- 缺点：FwFM金庸一个标量来表达域交互的强度，自由度不够，表达能力不强
####  2.1.3 特征构造——特征叫交叉——隐式交叉

显示交叉的局限性
- 对飞行线性的建模能力是有限的
- 很难扩展到更高阶的特征交叉
- 但数据稀疏很大时，模型训练比较困难
- 对所有特征交叉一视同仁，可能会限制模型的表达能力
- 不能自动化实现特征交叉

那为了解决这一问题，随着深度学习时代的带来，最容易想到的方法是直接用MLP来代替特征交叉项。因为可以简单认为MLP具有万能函数拟合的能力，所以区区特征组合，也可以用MLP来实现。在推荐系统的CTR模型，可以看到很多特征交叉和神经网络结合的模型。

但是，我们也不能认为MLP能够很好地做好这件事，尽管MLP在理论上具有万能函数拟合的能力。MLP这种特征交叉的能力是比较弱的，MLP不是天生为了进行特征交叉设计的。[3]

不过，可以采取一些措施来缓解这一问题。例如训练模型初始化、设计专门的特征交叉层、结合不同的模型（同时考虑缔结特征和高阶交叉特征）

隐式交叉的分类
- 结合不同的模型（比如Wide&Deep.DeepFM模型）
- 先交叉后MLP(例如：PNN模型，NFM模型)
- 并不是所有的特征交叉项都是重要的（比如：AFM模型）
- 自动化实现特征交叉（比如：DEEp&cross层，xDEEPFM模型）
- 特征交叉新方式（CAN）

##### 2.1.3.1 特征构造——特征叫交叉——隐式交叉_结合不同模型
**1. wide&deep**

https://blog.csdn.net/csdnnews/article/details/109396511
>- 简介

正如我们之前文章所分享的一样，推荐系统也可以看成是搜索的排序系统。它的输入是一个用户信息以及用户浏览的上下文信息，返回的结果是一个排好序的序列。

正因为如此，对于推荐系统来说，也会面临一个和搜索排序系统一个类似的挑战——记忆性和泛化性的权衡。记忆性可以简单地理解成对商品或者是特征之间成对出现的一种学习，由于用户的历史行为特征是非常强的特征，记忆性因此可以带来更好的效果。但是与之同时也会有问题产生，最典型的问题就是模型的泛化能力不够。

对于泛化能力来说，它的主要来源是特征之间的相关性以及传递性。有可能特征A和B直接和label相关，也可能特征A与特征B相关，特征B与label相关，这种就称为传递性。利用特征之间的传递性， 我们就可以探索一些历史数据当中很少出现的特征组合，从而获得很强的泛化能力。

在大规模的在线推荐以及排序系统当中，比如像是LR这样的线性模型被广泛应用，因为这些模型非常简单、拓展性好、性能很强，并且可解释性也很好。这些模型经常用one-hot这样的二进制数据来训练，举个例子，比如如果用户安装了netflix，那么user_installed_app=netflix这个特征就是1，否则就是0。因此呢，一些二阶特征的可解释性就很强。

比如用户如果还浏览过了Pandora，那么user_installed_app=netflix,impression_app=pandora这个联合特征就是1，联合特征的权重其实就是这两者的相关性。但是这样的特征需要大量的人工操作，并且由于样本的稀疏性，对于一些没有在训练数据当中出现过的组合，模型就无法学习到它们的权重了。

但是这个问题可以被基于embedding的模型解决，比如之前介绍过的FM模型，或者是深度神经网络。它可以通过训练出低维度下的embedding，用embedding向量去计算得到交叉特征的权重。然而如果特征非常稀疏的话，我们也很难保证生成的embedding的效果。比如用户的偏好比较明显，或者是商品比较小众，在这样的情况下会使得大部分的query-item的pair对没有行为，然而由embedding算出来的权重可能大于0，因此而导致过拟合，使得推荐结果不准。对于这种特殊的情况，线性模型的拟合、泛化能力反而更好。

在这篇paper当中，我们将会介绍Wide & Deep模型，它在一个模型当中兼容了记忆性以及泛化性。它可以同时训练线性模型以及神经网络两个部分，从而达到更好的效果。

论文的主要内容有以下几点：

1. Wide & Deep模型，包含前馈神经网络embedding部分以及以及线性模型特征转换，在广义推荐系统当中的应用

2. Wide & Deep模型在Google Play场景下的实现与评估，Google Play是一个拥有超过10亿日活和100w App的移动App商店

>- 推荐系统概述

[![2023-11-05-120302.png](https://i.postimg.cc/25PmXqY4/2023-11-05-120302.png)](https://postimg.cc/5HqZX260)

当用户访问app store的时候会生成一个请求，这个请求当中会包含用户以及上下文的特征。推荐系统会返回一系列的app，这些app都是模型筛选出来用户可能会点击或者是购买的app。当用户看到这些信息之后，会产生一些行为，比如浏览（没有行为）、点击、购买，产生行为之后，这些数据会被记录在Logs当中，成为训练数据。

我们看下上面部分，也就是从DataBase到Retrieval的部分。由于Database当中的数据量过大，足足有上百万。所以我们想要在规定时间内（10毫秒）给所有的app都调用模型打一个分，然后进行排序是不可能的。所以我们需要对请求进行Retrieval，也就是召回。Retrieval系统会对用户的请求进行召回，召回的方法有很多，可以利用机器学习模型，也可以进行规则。一般来说都是先基于规则快速筛选，再进行机器学习模型过滤。

进行筛选和检索结束之后，最后再调用Wide & Deep模型进行CTR预估，根据预测出来的CTR对这些APP进行排序。在这篇paper当中我们同样忽略其他技术细节，只关注与Wide & Deep模型的实现。
>-  Wide&Deep原理
[![2023-11-05-120622.png](https://i.postimg.cc/3whzyg3f/2023-11-05-120622.png)](https://postimg.cc/QF6fwK9c)
[![2023-11-05-120723.png](https://i.postimg.cc/KjCHmT9N/2023-11-05-120723.png)](https://postimg.cc/9wtppDHR)
[![2023-11-05-120819.png](https://i.postimg.cc/pLb67D5V/2023-11-05-120819.png)](https://postimg.cc/mtjj11SK)

**2. DeepFM**

https://blog.csdn.net/maqunfi/article/details/99635620

https://www.cnblogs.com/techflow/p/14260630.html

##### 2.1.3.2 特征构造——特征叫交叉——隐式交叉_线交叉后MLP
**1. PNN**

https://zhuanlan.zhihu.com/p/111842425

**2. NFM**

https://zhuanlan.zhihu.com/p/92293407
##### 2.1.3.3 特征构造——特征叫交叉——隐式交叉_并不是所有特征都是重要的
**1. AFM模型**

https://zhuanlan.zhihu.com/p/387221096

##### 2.1.3.4 特征构造——特征叫交叉——隐式交叉_自动化实现特征交叉
**1. deep&cross**

https://zhuanlan.zhihu.com/p/55234968

**2.Xdeep**

https://zhuanlan.zhihu.com/p/57162373

##### 2.1.3.5 特征构造——特征叫交叉——隐式交叉_特征交叉新方式
CAN: Revisiting Feature Co-Action for Click-Through Rate Prediction 这个工作提供了一种新的特征交互思路，在「特征工程上手动特征交叉」和「模型上自动特征交叉」之间做了折衷，也是「记忆性」和「泛化性」的互补。可以认为是开创了特征交互的新路线。具体参考以下链接：

https://www.deeplearn.me/3984.html

https://blog.csdn.net/m0_52122378/article/details/110383681


### 2.2 特征编码（针对的离散的文本型特征）
https://zhuanlan.zhihu.com/p/117230627

1. Oridinal Encoder

      1. 原理
         - 这是一种最简单的思路，对于一个具有m个category 的feature,我们将其映射到[0,m-1]的整数
         - 当然 Ordinal Encoding 更适用于 Ordinal Feature，即各个特征有内在的顺序。
         - 例如对于"学历"这样的类别，"学士"、"硕士"、"博士" 可以很自然地编码成 [0,2]，因为它们内在就含有这样的逻辑顺序。
         - 但如果对于”颜色“这样的类别，“蓝色”、“绿色”、“红色”分别编码成 [0,2].是不合理的，因为我们并没有理由认为“蓝色”和“绿色”的差距比“蓝色”和“红色”的差距对于特征的影响是不同的。
      2. 实现
         - 在具体实现的方式上，sklearn.preprocessing 两个类似的编码函数：LabelEncoder() 和 OrdinalEncoder() ，两者有一定的区别。
         - lable Encoder:LabelEncoder() 用于标签编码，而数据的标签一般是只有一维的（可以理解为一列），所以其编码的数据维度应为 (n_samples,)
      
               from sklearn.preprocessing import LabelEncoder
               le = LabelEncoder()
               # 编码
               le.fit(['ac', 'ef', 'bd', 'ac', 'ef'])
               # 输出类别
               print(le.classes_)                        
               # 编码
               print(le.transform(['ac', 'bd', 'ef', 'ac']))
               # 解码
               print(le.inverse_transform([0, 1, 2, 0, 1]))
               ['ac' 'bd' 'ef']
               [0 1 2 0]
               ['ac' 'bd' 'ef' 'ac' 'bd']
           - oridinal encoder:OrdinalEncoder() 用于特征编码，数据的特征一般是多维的（可以理解为多列），所以其编码的数据维度应为 (n_samples, n_features)

               from sklearn.preprocessing import OrdinalEncoder
               oe = OrdinalEncoder()
               oe.fit([ ['ac', 'ef'], 
                        ['ef', 'bd'],
                        ['gg', 'bd'],
                        ['ef', 'ff']]) # 必须是二维
               print(oe.categories_)
               print(oe.transform([ ['ef', 'ff'],
                                    ['gg', 'bd'] ]))
               print( oe.inverse_transform([ [0, 1], 
                                             [2, 1],
                                             [1, 2] ]))
               ['ac' 'bd' 'ef']
               [0 1 2 0]
               ['ac' 'bd' 'ef' 'ac' 'bd']
2. one-hot encoder(OHE)
- 优点
  - 解决了分类器不好处理属性数据的问题
  - 在一定程度上也起到可扩充特征的作用
- 缺点
  - 当特征取值较多的时候容易出现维度灾难，可以使用PACA等降维的方法进行降维
- 应用场景
  - 综上所述，OHE 一般会用于类别数较小且模型对数值大小较敏感(LR和SVM)的情况，且类别数一般不会取超过 
- 实现
  - 实现好的OHE有很多 ，其中Pandas 就自带了 pd.get_dummies() 函数实现 OHE 功能，当然 sklearn 中也有相应的 OneHotEncoder() 函数。
  - 而实际上，pd_get_dummies() 一般多用来处理元素值为字符的OHE ，而 OneHotEncoder() 一般多用来处理元素值为数值的OHE
  - pd.get_dummies()
    - 对全部字符值元素列进行编码
    
               import pandas as pd
               data = pd.read_csv('train.csv')
               a = data.iloc[1:5, 6:8]
               print(a)
               a = pd.get_dummies(a)
               print(a)
               nom_0      nom_1
               1  Green  Trapezoid
               2   Blue  Trapezoid
               3    Red  Trapezoid
               4    Red  Trapezoid
                  nom_0_Blue  nom_0_Green  nom_0_Red  nom_1_Trapezoid
               1           0            1          0                1
               2           1            0          0                1
               3           0            0          1                1
               4           0            0          1                1
    - 对指定列进行编码合并
   
               import pandas as pd
               data = pd.read_csv('train.csv')
               a = data.iloc[1:5, 6:9]
               print(a)
               a = a.join(pd.get_dummies(a.nom_2))
               print(a)
               nom_0      nom_1    nom_2
               1  Green  Trapezoid  Hamster
               2   Blue  Trapezoid     Lion
               3    Red  Trapezoid    Snake
               4    Red  Trapezoid     Lion
                  nom_0      nom_1    nom_2  Hamster  Lion  Snake
               1  Green  Trapezoid  Hamster        1     0      0
               2   Blue  Trapezoid     Lion        0     1      0
               3    Red  Trapezoid    Snake        0     0      1
               4    Red  Trapezoid     Lion        0     1      0
   - Onehot Encoder
     
https://blog.csdn.net/qq_35436571/article/details/96426582

                              #-*- coding: utf-8 -*-
               from sklearn.preprocessing import  OneHotEncoder
               
               enc = OneHotEncoder()
               enc.fit([[0, 0, 3],
                        [1, 1, 0],
                        [0, 2, 1],
                        [1, 0, 2]])
               
               ans = enc.transform([[0, 1, 3]]).toarray() 
                '''
                如果不加 toarray() 的话，输出的是稀疏的存储格式，即索引加值的形式，也可以通过参数指定 sparse = False 来达到同样的效果
                '''
               print(ans) 
               #输出 [[ 1.  0.  0.  1.  0.  0.  0.  0.  1.]]
               
**解释：**

对于输入数组，依旧是把每一行当作一个样本，每一列当作一个特征，

先来看第一个特征，即第一列 [0,1,0,1]，也就是说它有两个取值 0 或者 1，那么 one-hot 就会使用两位来表示这个特征，[1,0] 表示 0， [0,1] 表示 1，在上例输出结果中的前两位 [1,0…] 也就是表示该特征为 0；

第二个特征，第二列 [0,1,2,0]，它有三种值，那么 one-hot 就会使用三位来表示这个特征，[1,0,0] 表示 0， [0,1,0] 表示 1，[0,0,1] 表示 2，在上例输出结果中的第三位到第六位 […0,1,0,0…] 也就是表示该特征为 1；

第二个特征，第三列 [3,0,1,2]，它有四种值，那么 one-hot 就会使用四位来表示这个特征，[1,0,0,0] 表示 0， [0,1,0,0] 表示 1，[0,0,1,0] 表示 2，[0,0,0,1] 表示 3，在上例输出结果中的最后四位 […0,0,0,1] 也就是表示该特征为 3；
3. Traget encoder(mean encoder)

**3.1 背景**

我们当前处理的特征都是定性特征，而当定性特征的基数cardinality (即这个定性特征的类别数量)很大时，之前介绍的OE 和 OHE 都无法得到一个好的结果。

- OE 编码高基数定性特征，虽然只需要一列，但是每个自然数都具有不同的重要意义，对于Target而言线性不可分。
   - 使用简单模型，容易欠拟合，无法完全捕获不同类别之间的区别
   - 使用复杂模型，容易在其他地方过拟合
- OHE 编码高基数定性特征，会产生上万列的稀疏矩阵，使得训练起来更加困难，除非算法有相关优化(例如 SVM) ，否则一般不会使用。
- 所以此时我们会考虑使用Target Encoder(也叫Mean Encoder) ，对于C分类问题，进行 Target Encoder 后只需要增加C-1个feature列。当 C远远小于N,则相对于 OHE 可以节省很多内存。

**3.2 原理**

Target Encoder 是一种有监督的编码方式，适用于分类和回归问题。为了简化讨论，我们接下来只考虑分类问题。

假设在分类问题中，目标y一共有C个类别，具体的一个类别用target表示，为了简化讨论，我们当前考虑一个 
 target。

同时假设某一个定性特征variable中一共有K个不同的类别，具体的一个类别用小写的k表示。

**3.2.1 预备知识**

[![2023-11-04-231559.png](https://i.postimg.cc/ncD1CXnb/2023-11-04-231559.png)](https://postimg.cc/JGRXF4yP)

[![2023-11-04-231717.png](https://i.postimg.cc/RZ0KwqBZ/2023-11-04-231717.png)](https://postimg.cc/bsKZhyr7)

[![2023-11-04-232046.png](https://i.postimg.cc/Znpt7Tz9/2023-11-04-232046.png)](https://postimg.cc/n9FPMfzZ)
- 一般来说，模型对复杂、非线性的特征目标越依赖，Target Encoder越有效。例如树模型的深度有限，可以用 Target Encoder 来补偿它，可以用它的短板来获得更好的分数。

**3.2.3 问题**

刚刚的基本思路中，我们直接简单地用后验概率来进行编码，但实际上如果我们对整个训练集使用后验概率来进行编码，这显然是非常容易OverFitting 的。

因为我们直接对训练集使用后验概率来进行编码，这样得到的新的feature是和target也就是训练集的label是非常强相关的，即发生了Target Value Leak 。

我们可以在训练集上得到一个非常好的效果，但是到了验证集上的效果就会很差，过拟合会非常严重。

所以我们可以使用以下方法来进行改进优化，往往能得到非常好的结果。

**3.2.4 Regularization**

[![2023-11-04-232508.png](https://i.postimg.cc/v8pSg6vv/2023-11-04-232508.png)](https://postimg.cc/SjfGHR3X)
[![2023-11-04-232827.png](https://i.postimg.cc/fbnHCxTv/2023-11-04-232827.png)](https://postimg.cc/mPjN219c)

**K-fold**

K-Fold 交叉验证可以一定程度上缓解 Target Value Leak 的问题，其基本思想也是基于对某一部分特征值进行编码时不能泄露其对应的目标值。

所以我们可以将训练集乱序分成K份，例如这里进行五折交叉验证。
[![2023-11-04-232929.png](https://i.postimg.cc/X75d8tZT/2023-11-04-232929.png)](https://postimg.cc/0b8z5W0f)

假设我们当前要对 fold1 进行编码，我们使用 fold2 - fold5 的所有数据进行之前所说的计算，并使用其结果对 fold1 进行编码，这样的话没有直接地泄露 fold1 的target值。

同样的对于其他的每一折，都是用其他四折的数据进行计算然后再进行编码即可。

举一个简单例子，还是拿之前的表格中的a部分出来看看：
[![2023-11-04-233105.png](https://i.postimg.cc/pdcZm71y/2023-11-04-233105.png)](https://postimg.cc/1g66TWSZ)
[![2023-11-04-233151.png](https://i.postimg.cc/2jhQYVm1/2023-11-04-233151.png)](https://postimg.cc/WtpF6pdV)

**3.3 回归问题**
[![2023-11-04-233332.png](https://i.postimg.cc/rp4m4RfW/2023-11-04-233332.png)](https://postimg.cc/DW2nk0ky)

**3.4实现**

朴素的 Target Mean 以及 Expanding Mean 实现在之前已经给出，可以非常简洁地实现。

当然在具体使用中我们都会加入 K-Fold 以及 Regularization ，自己手写实现更麻烦一些且容易出错，所以这里我们使用 [scikit-learn-contrib][categorical-encoding][https://github.com/scikit-learn-contrib/category_encoders] 所实现的库函数来完成。

具体 API 的使用方法可参考文档：Target Encoder[http://contrib.scikit-learn.org/categorical-encoding/targetencoder.html]

接下来举一个非常简单的例子并进行计算，例如我们有以下数据example.csv：
[![2023-11-04-233829.png](https://i.postimg.cc/8PDNKnt7/2023-11-04-233829.png)](https://postimg.cc/XX2RXxMW)
接下来我们使用 5-Fold + 使用了3.2.4 中正则项的 Target Encoder 对齐进行编码，代码如下：

               from category_encoders import TargetEncoder
               from sklearn.model_selection import KFold
               import pandas as pd
               data = pd.read_csv('example.csv')
               labels = data.label
               data.drop('label', axis=1, inplace=True)
               kf = KFold(n_splits=5, shuffle=False)
               for train_idx, test_idx in kf.split(data):
                   print(train_idx, test_idx)
                   te = TargetEncoder(cols='feature').fit(data.iloc[train_idx], labels[train_idx])
                   to = te.transform(data.iloc[test_idx])
                   print(to)

得到最终的输出如下
               [2 3 4 5 6 7 8 9] [0 1]
                  feature  label
               0   0.3234      0
               1   0.3234      1
               [0 1 4 5 6 7 8 9] [2 3]
                   feature  label
               2  0.494072      0
               3  0.255928      0
               [0 1 2 3 6 7 8 9] [4 5]
                  feature  label
               4     0.25      1
               5     0.25      0
               [0 1 2 3 4 5 8 9] [6 7]
                   feature  label
               6  0.494072      0
               7  0.255928      0
               [0 1 2 3 4 5 6 7] [8 9]
                   feature  label
               8  0.029801      0
               9  0.029801      1
[![2023-11-04-234103.png](https://i.postimg.cc/kGqzcLq9/2023-11-04-234103.png)](https://postimg.cc/dkN5QHhS)
**4.catboost encoder**

**5. cyclic features**

**5.1 原理**

接下来考虑一下题目中所说的循环特征，例如”月份”、“星期”，它们不是简单的 Oridinal Feature ，它们还具有循环的特性，例如十二月和一月也是相邻的月份，所以他们也应该有着某种联系。

所以这时候我们可以引入均匀的循环特征，例如我们采用极坐标系上的角度来描述循环特征的每个位置，而角度可以用sin值和cos值唯一确定。

**5.2实现**

               data = pd.read_csv('train.csv')
               
               for col in ['day', 'month']:
                   data[col + '_SIN'] = np.sin((2 * np.pi * data[col])) / max(data[col])
                   data[col + '_COS'] = np.cos((2 * np.pi * data[col])) / max(data[col])  
               data.drop(['day', 'month'], axis=1, inplace=True)

**6 其他**

其他编码方法还有很多，例如feature hashing,leaveout encoder等方法

## 3. 特征提取
- 定义
  - 可能由于特征矩阵过大，一些样本如果直接使用预测模型算法可能在原始数据中有太多的列被建模，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。特征提取是一个自动化的降维过程。使得特征太多的样本被建模的维数降低。
- 作用
  - 最大限度地降低数据的维度的前提下能够同时保证保留目标的重要的信息，特征提取涉及到从原始属性中自动生成一些新的特征集的一系列算法，降维算法就属于这一类。特征提取是一个自动将观测值降维到一个足够建模的小数据集的过程。 ​ 数据降维有以下几点好处： ​
  
    - 1、避免维度灾难，导致算法失效，或者时间复杂度高 ​
    - 2、避免高维数据中引入的噪声，防止过拟合 ​
    - 3、压缩存储，可视化分析
### 3.1 特征提取_方法
- 列表数据
  - 使用的方法包括一些投影方法，像主成分分析和无监督聚类算法。
- 图形数据
  - 可能包括一些直线检测和边缘检测，对于不同领域有各自的方法
  - 特征提取的关键点在于这些方法是自动的（只需要从简单方法中设计和构建得到），还能够解决不受控制的高维数据的问题。大部分的情况下，是将这些不同类型数据（如图，语言，视频等）存成数字格式来进行模拟观察。 ​ 不同的数据降维方法除了实现降维目标的作用，同时具有各自的特点，比如主成分分析，降维后的各个特征在坐标上是正交；非负矩阵分解，因为在一些文本，图像领域数据要求非负性，非负矩阵分解在降维的同时保证降维后的数据均非负；字典学习，可以基于任意基向量表示，特征之间不再是独立，或者非负；局部线性嵌入，是一种典型的流型学习方法，具有在一定邻域保证样本之间的距离不变性。
[![2023-11-04-214639.png](https://i.postimg.cc/MTMdZhgY/2023-11-04-214639.png)](https://postimg.cc/k6CxfhJV)
>-  线性降维
>  >- 常见的降维方法除了基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。
1. 主成分分析
   - 选择方差最大的K个特征[无监督] 使用decomposition库的PCA类选择特征的代码如下：

          from sklearn.decomposition import PCA
          #主成分分析法，返回降维后的数据
          #参数n_components为主成分数目
          PCA(n_components=2).fit_transform(iris.data)
2. 线性判别分析法（LDA）
   - 选择分类性能最好的特征[有监督] 使用lda库的LDA类选择特征的代码如下：
   
          from sklearn.lda import LDA
          #线性判别分析法，返回降维后的数据
          #参数n_components为降维后的维数
           LDA(n_components=2).fit_transform(iris.data, iris.target)
3. 局部嵌入
4. 拉普拉斯特征映射/LE
5. 随机邻域嵌入/SNE
6. t-分布邻域嵌入/T-SNE
>- 非线性降维
1. 核主成分分析（KPCA）:带核函数的PCA
   - 局部线性嵌入（LLE):利用流行结构进行降维，看还有拉普拉斯图，MDS等
>- 迁移学习将为
1. 迁移成分分析（TCA）:不同邻域之间迁移学习降维
## 4. 入模评估
1. 作用

模型评估一有两个目的：
- 检验特征工程的工作，查看所选择的特征是否有利于提升模型的性能
- 检查参数调整工作，通过调整模型参数，找到最佳参数使得模型的分类、预测性能最佳

特征工程需要在机器学习的应用中加深理解一个完整的特征工程、机器学习的处理过程是： 应用机器学习的过程包含许多步骤。从问题的定义，到数据的选择和准备，以及模型的准备，模型的评价和调参，最后是结果的表达。这个过程中与我们的话题相关的部分可以用下面几步描述： 
1. 选择数据：整合数据，规范化到数据集中，集中数据
2. 预处理数据：格式化，清理，采样
3. 转换数据：特征工程要做的事情
4. 建模数据：建立模型，评价模型，调整模型

我们看到紧随着特征工程就是建模。这表明，我们做特征工程需要与模型，表现度量相结合。同时也表明，我们需要留下那些适合建模的数据。比如说在最后一步规范化和标准化数据。这看起来是一个预处理的步骤，但实际上他帮助我们意识到对于一个有效模型需要什么样的最终形态。

## 4.召回
https://zhuanlan.zhihu.com/p/145645116

https://zhuanlan.zhihu.com/p/115690499
### 4.1 深入理解youtube推荐系统算法
https://zhuanlan.zhihu.com/p/114703091

- youtube推荐系统的三大难点
  - 数据规模：YouTube 的用户和视频量级都是十亿级别的，需要分布式学习算法和高效的部署。
   - 新颖性：推荐系统需要及时对新上传的视频和用户的新行为作出响应。
   - 数据噪音：由于稀疏和外部因素影响，用户的历史行为很难预测。大部分 YouTube 视频只有隐式反馈（即用户对视频的观看行为），缺少显式反馈（即用户对视频的评分）。此外，视频的元信息不够有结构性。我们的算法需要对训练数据的这些因素稳健（robust）。
- 推荐系统的三个过程
  - 第一部分 召回网络：此阶段主要目的是从百万级的视频中检索出一小部分的视频用于之后的排序操作，这部分需要处理的数据量非常大，速度要求快，所有使用的模型和特征都不能太复杂。召回网络会根据用户的历史信息（比如用户的历史观看、搜索等）进行召回，这一阶段召回的视频满足用户泛化的兴趣，用户之间的相似度则通过粗略的特征来表示，如用户观看视频的ID，搜索query和用户画像。
  - 第二部分 排序网络：此阶段会使用更加丰富和精细的用户和视频特征，预测用户对召回的视频打分，然后根据分数进行排序，依次展示给用户。这部分最主要是能够精准的将视频推送给用户，所以需要更加复杂的模型和特征来提升推荐效果。
  - 第三部分 线下评估：评估指标有precision、recall、ranking loss等，最终的效果还是需要线上做A/B测试，关注的指标包括：点击率、观看时间等；需要指出的是，线上线下有时候并不能保持一致的结果。
>- matching
- 传统召回思路
  - 先离线计算好商品的embedding以及用户的embedding，线上召回的时候，根据用户的embedding去和所有商品的embedding内积，找出topN.这种传统的方式需要解决以下几个问题
    - 商品的Embedding空间和用户的Embedding空间如何保证在同一个空间。
    - 需要计算与所有商品的内积，存在性能问题。
    - 诸如奇异值分解的方法，输入协同矩阵，特征比较单一。
- 问题建模
[![2023-11-05-162303.png](https://i.postimg.cc/X7JKMyNC/2023-11-05-162303.png)](https://postimg.cc/14kVwXqm)
- 负类采样（重要性采样softmax）

在每次计算中，softmax的分母，都需要遍历视频库
 中所有的视频，并且用户上下文向量与视频向量之间的点积，exp等操作造成计算量过大，因此如何高效训练成为一个问题。其解决方法采用了负采样机制（sample negative classes ）提升训练速度，并使用重要性加权（importance weighting）的方式校正这个采样。对于每个样本，对真实标签和采样得到的负类，最小化其交叉熵损失函数。相比经典 Softmax，这有几百倍的速度提升。

>- 召回模型网路结构

在做NLP任务时，如何将文本或者文本中的一字一句，表示成结构化的，计算机能够理解的形式是第一步。经常采用方法的就是word2vec，就是将所有的word表示成低维稠密的embedding向量，最后将词的embedding向量喂给神经网络进行学习。
[![2023-11-05-162654.png](https://i.postimg.cc/3rPGqfqP/2023-11-05-162654.png)](https://postimg.cc/nMGMj2pk)
Youtube的召回模型也受此启发，采用了word embedding的技巧来计算每一个视频的embedding，然后将视频的embedding，用户搜索视频的embedding分别计算average，再加入用户的属性、视频质量等特征，采用两个完全相连的ReLU层和softmax函数来预测用户下一个看的视频是什么。

使用DNN的原因之一，在DNN中连续性变量和类别型变量都很容易输入到模型中，包括一些人口统计特征（Demographic features），对最终的效果起着十分重要的作用。用户的地域，设备等都可以作为embedding向量输入到DNN中去。简单的二值化特征（如性别）和数值型特征（如年龄）可以直接输入到DNN中，数值型需要经过归一化到[0,1]再输入到模型中。

1. 输入层
- 用户观看视频序列ID：对视频ID的embedding向量进行mean pooling，得到视频观看向量（watch vector）。
- 用户搜索视频序列ID：对视频ID的embedding向量进行mean pooling，得到视频搜索向量（search vector）。
- 用户地理特征和用户设备特征：均为一些离散特征，可以采用embedding方法或者直接采用one-hot方法（当离散维度较小时），得到用户的地理向量和设备向量
- Example Age：在推荐系统中很重要的一点是视频的新颖性，用户更倾向于观看新视频，但机器学习模型都是基于历史观看视频记录学习，所以在某种程度上，模型和业务是相悖的，所以文中构建了一个特征example age，简单的可以理解为是视频的年龄，初始值设为0，随着时间的增长，记录视频的年龄。加入之后效果十分明显，如图所示
[![2023-11-05-163337.png](https://i.postimg.cc/TwJssWds/2023-11-05-163337.png)](https://postimg.cc/WFtSFzJm)

人口属性特征：用于提供先验，使其对新用户也能做出合理的推荐。具体来说，对用户的地理区域和使用的设备进行 Embedding 并将特征进行拼接（concatenation）
3. MLP层
使用常见的塔状设计，底层最宽，往上每层的神经元数目减半，直到 Softmax 输入层是 256 维（1024ReLU->512ReLU->256ReLU）。
4. softmax层
5. 输出层
[![2023-11-05-165910.png](https://i.postimg.cc/nz13cbQS/2023-11-05-165910.png)](https://postimg.cc/MMv7dL51)
- 召回 优化
  - 在线服务阶段，通过视频向量V和用户向量u进行相似度计算，为了满足时延要求，在进行实际的召回计算时采用的是最近邻查询的方式。对于每个用户向量u，对视频库中的所有视频根据向量v做最近邻算法，得到top-N的视频作为召回结
- 样本选择和上下文选择
  - 在有监督学习问题中，最重要的选择是label了，因为label决定了你做什么，决定了你的上线，而feature和model都是在逼近label，我们的几个设计如下：
  - 使用更广的数据源：不仅仅使用推荐场景的数据进行训练，其他场景比如搜索等的数据也要用到，这样也能为推荐场景提供一些explore
  - 为每个用户生成固定数量训练样本：我们在实际中发现的一个practical lessons，如果为每个用户固定样本数量上限，平等的对待每个用户，避免loss被少数active用户domanate，能明显提升线上效果
  - 抛弃序列信息：我们在实现时尝试的是去掉序列信息，对过去观看视频/历史搜索query的embedding向量进行加权平均。这点其实违反直觉，可能原因是模型对负反馈没有很好的建模
  - 不对称的共同浏览（asymmetric co-watch）问题：所谓asymmetric co-watch值的是用户在浏览视频时候，往往都是序列式的，开始看一些比较流行的，逐渐找到细分的视频。下图所示图(a)是hled-out方式，利用上下文信息预估中间的一个视频；图(b)是predicting next watch的方式，则是利用上文信息，预估下一次浏览的视频。我们发现图(b)的方式在线上A/B test中表现更佳。而实际上，传统的协同过滤类的算法，都是隐含的采用图(a)的held-out方式，忽略了不对称的浏览模式。
>- ranking
>- 
### 4.2 召回_背景介绍
[![2023-11-05-152407.png](https://i.postimg.cc/jjQWB8yh/2023-11-05-152407.png)](https://postimg.cc/jW2d74gW)
召回是推荐系统的第一阶段，主要根据用户和商品的部分特征，从海量的物品库里快速找回一小部分用户潜在感兴趣的物品，然后交给排序环节。这部分需要处理的数据量非常大，速度要求快，所有使用的策略、模型和特征都不能太复杂。下面主要介绍赛中常见的召回算法
- 基于内容的召回：使用item之间的相似性来推荐与用户喜欢的item相似的item
  - 例如：如果用户A看了《绣春刀2》这部杨幂主演的电影后，则会为他推荐杨幂主演的其他电影或电视剧
- 协同过滤：同时使用query和item之间的相似性来进行推荐
  - 例如：如果用户A看了《绣春刀2》这部杨幂主演的电影后，则会为他推荐杨幂主演的其他电影或电视剧
- 基于FM模型的召回：FM是基于矩阵分解的推荐算法，其核心是二阶特征组合
- 基于深度神经网络的方法：利用深度网络生成相应的候选集
### 4.3召回_基于内容的召回
[![2023-11-05-153511.png](https://i.postimg.cc/HWKQwhzJ/2023-11-05-153511.png)](https://postimg.cc/BtTLsNds)
基于内容的召回（ CB召回 ），一般也叫做标签召回。当谈起CB的时候，大家可能会觉的很简单，用tag或者用cate召回就行了，好像没什么可做的。可事实上，CB并不仅仅是用tag和cate做个倒排就搞定了。这类召回的核心思想是基于item自身的属性，这些属性可以表达为tag，Cate，也可以用来表达用户ID，用户类型等，更可以通过⼀些交叉验证的⽅式，针对内容提取向量，将内容表达为连续向量的方式进行召回。接下来我们进一步来理解基于内容的过滤。

在实际的应用中，如电影推荐，首先我们根据用户之前的历史行为信息（如点击，评论，观看等），CB会使用item相关特征来推荐给用户与之前喜欢的item类似的item。为了更形象的表示CB，假设一个应用商店要推荐给用户相应的APP。下图是相应的特征矩阵，其中每一行代表一个应用程序，每一列代表一个特征。包括不同的类别特征，应用程序的发布者信息等。为简化起见，假定此特征矩阵是布尔类型的的：非零值表示应用程序具有该特征。
[![2023-11-05-154003.png](https://i.postimg.cc/sgZyBvXQ/2023-11-05-154003.png)](https://postimg.cc/JsmSv7jM)
还可以在同一特征空间中表示用户。一些与用户相关的特征可以由用户明确提供。例如，用户在其个人资料中选择“娱乐应用”。其他特征可能是隐式的，具体取决于它们先前安装的应用程序。例如，用户安装了由Science R Us发布的另一个应用程序。

模型应推荐与此用户有关的item。为此，必须首先选择一个相似性指标（如，点积）。然后，推荐系统会根据此相似性度量标准为每个候选item打分。请注意，建议是针对该用户的，因为该模型未使用其他用户的任何信息。

- 基于内容召回的优点
  - 该模型不需要其他用户的任何数据，因为推荐是针对该用户的。这使得更容易扩展到大量用户
  - 该模型可以捕获用户的特定兴趣
- 基于内容召回的缺点
  - 由于item的特征表示在某种程度上是手工设计的，因此该技术需要大量的领域知识。因此，模型很依赖手工设计特征的好坏
  - 该模型只能根据用户的现有兴趣提出建议。换句话说，该模型扩展用户先有兴趣的能力有限。

基于内容的召回看似比较容易，如果当我们的item属性越来越多的时候，比如⼀个视频可能有多个平行的tag以及其它属性，那么，为了把这些信息综合利用起来，我们还会利用多term检索的方式，去提升CB的效果。下面，我们针对这些内容详细的来聊⼀聊CB的常见优化点。
- 倒排优化
  - 优化倒排的主要目的是提升cb召回的推荐效果，常见的倒排基本是和线上排序指标⼀致的，比如，如果排序的指标是点击率，倒排理所应当，也是点击率。但这样的排序方式有个小问题，因为倒排排序使用的是后验的值，而排序通常也是单指标排序，这样，就很容易出现我们之前提到的，单指标被hack的问题，比如，用点击率倒排，头部都是标题党等。所以，这个问题是需要额外注意的。另外，也要考虑指标的bias的问题，例如，用完成率倒排导致短的视频都排到了头部。这种问题可以通过归一化的方式缓解。但有⼀个潜在风险，资源的后验表现的分布往往会跟资源本身的类型有关。
- 触发key的优化
  - key的优化要求只有⼀点，保证每次选择的key，是用户点击概率最⼤的key即可，所以通用的方式是把用户的点击历史按照属性加和取top，比如，在某个类别上点击的次数排序，把点击次数最多的那几个类别留作触发的key，这个过程很简单，没太多优化点，我们就不继续讨论了。这⾥想聊的是关于用户不⼀样的行为差异化权重的问题，我们选取key的过程实际是判断用户对某⼀类内容感兴趣的过程，也就是通过行为，来判断用户的感兴趣程度，在只有浏览功能的产品⾥，点击就是表达用户兴趣的唯一行为，但产品通常会设计很多交互功能，来帮助用户进行有效的兴趣表达，所以，在触发key的选取的时候需要考虑到这⼀点，至于怎么做是和业务形态相关的事，这⾥就不展开了。
- 多维度属性
  - 最后，我们来讲⼀下cb⾥⾯比较高阶的问题，多term的问题。我们先考虑这样⼀种情况，一个视频有很多个tag，tag的特性是平行不唯⼀。我们在线通过点击量汇聚得到了用户的高频点击tag，通常的方式是，每⼀个tag都会有⼀个倒排，然后各自召回。但很容易想到的，当用户同时有“帅哥”，“萌宠”这两个tag的时候，通过萌宠给用户召回⼀个美女+萌宠的视频显然没有召回⼀个帅哥+萌宠的视频更有吸引力，也就是说，我们在召回的时候，如果能同时考虑多个term之间的关系，会更有效⼀些。传统搜索⾥对于多term召回是有⼀套实现方式的，通过设置每个term的权重，可以在返回结果⾥去得到包含多term的结果，这部分属于搜索架构的内容范畴，我在此不展开了，有兴趣的同学可以找身边做搜索的同学了解⼀下。我们讲⼀个更和推荐match的方式，用户身上有多个标签，内容上面也有多个标签，我们在做多term匹配的时候，就是标签的list到标签的list，使得他们点击的概率最大即可。是不是觉得豁然开朗了？是不是转化成点击率建模问题了？更简单地，直接使用word2vec，把用户标签和内容标签作为⼀个sentence训练，再离线把内容的标签加和表征为内容的属性向量，在线做召回即可。当然，也可以⽤更复杂的⽅式来做，提升精度，大同小异，就不展开说了。那作者，内容的其他纬度等信息也是可以⼀样的方式加进去的，这就留给大家来讨论了。
### 4.4召回_协同过滤
为了解决基于内容的找回所存在的弊端，人们提出了协同过滤召回方式（CF）,CF同时使用user和item之间的相似性进行推荐。这样可以提高模型的推荐扩展性。也就是说，协同过滤模型可以根据相似用户B的兴趣像用户A推荐商品。此外，可以自动学习embedding，而无需依赖手工设计的特征

一般来说，协同过滤推荐分为三种类型。第一种是基于用户的协同过滤，第二种是基于项目的协同过滤，第三种是基于模型的协同过滤
- 基于用户(user-based)的协同过滤：主要考虑的是用户和用户之间的相似度，只要找出相似用户喜欢的物品，并预测目标用户对对应物品的评分，就可以找到评分最高的若干个物品推荐给用户。
- 基于项目(item-based)的协同过滤：和基于用户的协同过滤类似，只不过这时我们转向找到物品和物品之间的相似度，只有找到了目标用户对某些物品的评分，那么我们就可以对相似度高的类似物品进行预测，将评分最高的若干个相似物品推荐给用户。比如你在网上买了一本机器学习相关的书，网站马上会推荐一堆机器学习，大数据相关的书给你，这里就明显用到了基于项目的协同过滤思想。
- 基于模型（model based）的协同过滤：是目前最主流的协同过滤类型了，所含算法是非常之多的，如矩阵分解、关联算法、聚类算法、深度学习、图模型等等。

这里我们带来一个有关电影推荐系统的简单例子帮助更好的理解协同过滤。首先，考虑一个电影推荐系统，其中训练数据由一个反馈矩阵组成，其中每行代表一个user，每一列代表一个item。

关于电影的反馈分为以下两类：
- 显示反馈：用户通过提供评分来指定他们对特定电影的喜欢程度
- 隐式反馈：如果用户观看电影，则系统会推荐用户感兴趣的

假设反馈矩阵是布尔类型的，即值为1和0分别表示对电影是否感兴趣，当用户访问首页时，系统会根据以下两种情况推荐电影：

与用户过去喜欢的电影相似（Item-Based CF）

类似用户喜欢的电影（User-Based CF）

为便于举例阐述，让我们手工设计用以描述电影的一些特征：

[![2023-11-05-174818.png](https://i.postimg.cc/bwjnTzNZ/2023-11-05-174818.png)](https://postimg.cc/PP2NfnDH)

>- 一维embedding

假设我们为每部电影分配一个标量，用于描述该电影是适合儿童（负值）还是适合成人（正值）观看。 假设我们还为每个用户分配了一个标量，用于描述用户对儿童电影（接近-1）或成人电影（接近+1）的兴趣。 对于我们希望用户喜欢的电影，电影Embedding和用户Embedding的乘积应更高（接近1）。

[![2023-11-05-175423.png](https://i.postimg.cc/ZYvhSN6y/2023-11-05-175423.png)](https://postimg.cc/pyvSQpG2)

>- 二维embedding

一个特征不足以解释所有用户偏好。为了克服这个问题，让我们添加第二个特征；每部电影是上也流行篇或是小众文艺片上的表现程度。通过这个特征，我们现在可以使用下一二维embedding来表示每部电影
[![2023-11-05-180736.png](https://i.postimg.cc/GmCW1Sj4/2023-11-05-180736.png)](https://postimg.cc/ZCw78H2m)

当模型自动学习Embedding时，这种方法的协同性质就显而易见了。假设电影的Embedding向量是固定的。然后，模型可以为用户学习Embedding向量，以最好地解释他们的偏好。 因此，具有相似偏好的用户的Embedding将紧密在一起。同样，如果用户的Embedding是固定的，则我们可以学习电影Embedding以最好地解释反馈矩阵。结果，类似用户喜欢的电影的Embedding将在Embedding空间中紧密在一起。
>- 基于模型的协同过滤

[![2023-11-05-181032.png](https://i.postimg.cc/NMm8YbD0/2023-11-05-181032.png)](https://postimg.cc/njV9G468)
>- 选择目标函数

 一种直观的目标函数是距离平方，即在所有观察到的矩阵向上最小化平方误差之和：

[![2023-11-05-181032.png](https://i.postimg.cc/NMm8YbD0/2023-11-05-181032.png)](https://postimg.cc/njV9G468)

>- 最小化目标函数

- 随机梯度下降
- 加权交替最小二乘
>- 协同过滤的优缺点
- 优点
  - 无需领域知识：不需要相关领域知识，因为embedding是自动学习的
  - 发觉用户兴趣：该模型可以帮组用户发现新的兴趣点。系统可能并不知道用户对给定item的兴趣度，但是模型仍会推荐给他，因为相似的用户有着相同的兴趣点
  - 很好的初始模型：在魔种程度上，该方法仅需要反馈矩阵即可训练矩阵分解模型。而且该方法不需上下文特征。实际上，该方法可以用作多个召回队列中的一个，
- 缺点
  - 冷启动问题：模型预测结果是给定的（用户，商品）相应Embedding的点积。因此，如果在训练数据中item从未出现过，则系统也无法计算其Embedding，也无法得到相应的预测结果。此问题通常称为冷启动问题。但是，以下技术可以在某种程度上解决冷启动问题：
    - 利用EWALS进行预测
   [![2023-11-05-181901.png](https://i.postimg.cc/8zYnzKRd/2023-11-05-181901.png)](https://postimg.cc/nXv1d1wM)
### 4.5召回_基于FM模型召回
- FM模型的优点
  - 能够处理数据高度稀疏场景，SVM则不能
  - 具有线性的计算复杂度，而SVM依赖于support vector
  - FM能够在任意的实数特征向量中生效

FM结构如下
[![2023-11-05-182918.png](https://i.postimg.cc/6q7fgwPD/2023-11-05-182918.png)](https://postimg.cc/ZvSNyk4c)
FM特征数据结构：User相关、Item相关、类别相关的特征、历史行为数据特征等等，最后一列可看作是User对Item评分。FM通过不同特征的组合，生成新的含义。然而，特征组合也随之带来一些问题：
- 特征之间两两组合容易导致维度灾难；
- 组合后的特征未必有效，可能存在特征冗余现象；
- 组合后特征样本非常稀疏，如果原始样本中不存在对应的组合，则无法学习参数，那么该组合就显得无效
>- 具体召回过程

第一步，对于某个用户，我们可以把属于这个用户子集合的特征，查询离线训练好的FM模型中这个用户对应的特征embedding向量（FM模型求解出的隐向量，即Vi，其长度为k，包含k 个描述特征的因子），然后将这个用户对应的n个特征embedding向量累加，形成这个用户的兴趣向量U，这个向量维度和每个特征的维度是相同的。

类似的，我们也可以把每个物品，其对应的物品子集合的特征，查询离线训练好的FM模型对应的特征embedding向量，然后将m个物品子集合的特征embedding向量累加，形成物品向量I，这个向量维度和每个特征的维度也是是相同的。
[![2023-11-05-200802.png](https://i.postimg.cc/Sx2s26Nh/2023-11-05-200802.png)](https://postimg.cc/Sn4qHzMT)
第二步，对于每个用户以及每个物品，我们可以利用步骤一中的方法，将每个用户的兴趣向量离线算好，存入在线数据库中比如Redis（用户ID及其对应的embedding），把物品的向量逐一离线算好，存入Faiss(Facebook开源的embedding高效匹配库)数据库中，进行knn索引，然后高效检索。

第三步，当用户登陆或者刷新页面时，可以根据用户ID取出其对应的兴趣向量embedding，然后和Faiss中存储的物料embedding做内积计算，按照得分由高到低返回得分Top K的物料作为召回结果。
>- 矩阵分解和FM
[![2023-11-05-201045.png](https://i.postimg.cc/PJpcz5LP/2023-11-05-201045.png)](https://postimg.cc/q6rjp0xT)
可以认为FM是加了特征的矩阵分解（MF），原来用户和物品侧都只有一个id特征，现在用户侧加了年龄、性别、学历等特征，物品侧加了品类、店铺等特征，然后进一步融入到FM模型后，它将所有的特征转化为embedding低维向量表达，然后用户侧的特征和物品侧特征两两矩阵分解，即两两特征embedding的内积，得到特征组合的权重。
### 4.6 召回——基于深度神经网络模型
前文讲述了如何使用矩阵分解来学习embedding.矩阵分解的一些限制包括：
- 使用附加特征（即queryID /itemID以外的其他特征）困难。 因此只能对训练集中存在的用户或item进行推荐。
- 推荐的相关性。 正如前文所描述的那样，倾向于向所有人推荐热门item，尤其是在使用点积作为相似性度量时。 难以刻画特定的用户兴趣。

深度神经网络（DNN）模型可以解决矩阵分解的这些限制。 DNN可以轻松地融入query特征和item特征（由于网络输入层的灵活性），这可以帮助捕获用户的特定兴趣并提高推荐的相关性。

**4.6.1 softmax DNN模型**

一般而言，DNN模型是利用softmax作为最后一层的输出，它会将问题视为多分类问题，其中：

- 输入是用户query。
- 输出是一个概率向量，其大小等于语料库中item的数量，代表与每个item进行交互的概率； 例如，点击或观看视频的可能性
>- 输入层

DNN的输入可以包括
- 稠密（dense）特征（如点击率、观看时长）
- 稀疏（sparse）特征（如,观看视频类型、地区）

与矩阵分解方法不同，可以添加年龄或地区等附加特征。 我们用x表示输入向量。
[![2023-11-05-201902.png](https://i.postimg.cc/CLjNkqvW/2023-11-05-201902.png)](https://postimg.cc/qzvydzHL)
>- 模型结构
[![2023-11-05-202141.png](https://i.postimg.cc/nLTS7DLH/2023-11-05-202141.png)](https://postimg.cc/f3tvQkXF)
>- 输出层：预测概率分布
[![2023-11-05-202236.png](https://i.postimg.cc/vmqFyjfL/2023-11-05-202236.png)](https://postimg.cc/cKn5MFDr)

>- softmax embedding
[![2023-11-05-202633.png](https://i.postimg.cc/J44FrK7p/2023-11-05-202633.png)](https://postimg.cc/dDp6n2yC)
最后，定义用以比较以下两项的损失函数

**4.6.2 DNN和矩阵分解**

## 5. bert
https://www.elecfans.com/d/1308352.html

bert输入的是一句话，输出的是这句话中每个词元的词向量，不同段话的词元个数不一定相同。那么如何得到相同的大小的embedding呢？

- 可以采用三种pooling策略,SBERT在BERT/RoBERTa的输出结果上增加了一个pooling操作，从而生成一个固定大小的句子embedding向量。实验中采取了三种pooling策略做对比：
  - 直接采用CLS位置的输出向量代表整个句子的向量表示。
  - MEAN策略，计算各个token输出向量的平均值代表句子向量（平均是指在所有词元的词向量表示中做平均，比如，所有词元的第一个维度做平均）
  - MAX策略，取所有输出向量各个维度的最大值代表句子向量

在三种策略中，mean pooling策略是最好的。
## 5用户画像
### 5.1 用户画像基础
#### 5.1.1 用户
想要搞清楚用户画像，首先要搞清楚用户比如
- 你的用户的特征是什么？
  - 使用（购买）你的产品（服务）的全体
- 怎么描述用户的需求
  - 商业的七点
- 不同阶段的用户特征又是什么？
  - 用户的生命周期
    - 分五个部分：获取、激活、留存、变现、推荐
    - 分八个部分：认知、接触、使用、首单、复购、习惯、分享和流失

关于用户还有用户关键路径、用户旅程、用户决策过程等等
#### 5.1.2用户画像的概念
怎样区分User Portrait（用户肖像）、Customer Segment（用户细分）、User Persona（用户角色）、User Profile（用户画像）？
[![2023-11-05-204335.png](https://i.postimg.cc/hPmhy8Tt/2023-11-05-204335.png)](https://postimg.cc/LnHHh1xc)
[![2023-11-05-204423.png](https://i.postimg.cc/907XkV4n/2023-11-05-204423.png)](https://postimg.cc/9wcj46Yd)
- 用户画像是刻画用户需求的模型
- 用户画像是一种公共语言，串联互联网商业的高层、产品、开发、市场、运营等，提高沟通效率
#### 5.1.3用户画像的要素
用户画像的三要素：人、物、环境
[![2023-11-05-204706.png](https://i.postimg.cc/FsGH19t0/2023-11-05-204706.png)](https://postimg.cc/ykgHbC8N)
#### 5.1.4用户画像的关系
用户画像描述的是用户与物品的关系
[![2023-11-05-204805.png](https://i.postimg.cc/fTgQ6knq/2023-11-05-204805.png)](https://postimg.cc/hfxYmDGV)
#### 5.1.5用户画像的意义
用户画像广泛应用在推荐系统、广告系统、商业分析、数据分析、用户增长、数据运营、精准营销、量化风控等领域。
[![2023-11-05-204941.png](https://i.postimg.cc/tgjSfHTm/2023-11-05-204941.png)](https://postimg.cc/zyxFb6Tn)

### 5.2 用户画像原理
#### 5.2.1用户画像的方法
用户画像的主流方法：用户标签化

- 标签的定义
  - 标签是用户属性、兴趣、行为等特征的抽象与描述
- 标签的关系
  - 层次关系、序列关系、几何关系、独立关系
- 标签的分类
  - 人工标签、机器标签
  - 属性标签、兴趣标签、行为标签
  - 分层标签、分群体标签、个性化标签

从分层标签、分群标签到个性化标签，正是一个由粗到细的过程
[![2023-11-05-205959.png](https://i.postimg.cc/dt7TMkZL/2023-11-05-205959.png)](https://postimg.cc/SJhKCKrh)
#### 5.2.2分层标签
- 层
  - 将总体中各个用户按照某种特征分成若干个互不重叠的几个部分，每一部分叫做层
- 分层标签
  - 根据分层规则，对用户进行分层而打的标签
- 用户分层
  - 居于分层标签描述用户
  
** 5.2.2.1AARRR用户分层模型**

举个用户分层的例子：AARRR 用户分层模型。

AARRR 模型是由Acquisition（获取）、Activation（激活）、Retention（留存）、Referral（推荐）、Revenue（变现）等五个部分组成，形成一个用户流量漏斗。

按“从获取的用户到最终推荐的用户的演进路线”进行分层如下：

[![2023-11-05-210330.png](https://i.postimg.cc/MH3PnMzn/2023-11-05-210330.png)](https://postimg.cc/9wTtSfR2)

**5.2.2.2用户业务分层**

实际应用中，也可以基于具体业务进行用户分层。

比如：今日头条可以按资讯类别把用户分成科技用户、娱乐用户、游戏用户等等。

[![2023-11-05-210454.png](https://i.postimg.cc/QMjQrNrw/2023-11-05-210454.png)](https://postimg.cc/Snt26hRf)

将同一层的用户继续切分以满足精细化要求

#### 5.2.3分群标签
- 群
  - 按照制定规则，将总体中若干个用户合并为组，这样的组称为群
- 分群标签
  - 根据分层规则，对用户进行分群而打的标签
- 用户分群
  - 基于分群标签描述用户
    
**5.2.3.1 RFM分群用户模型**

RFM模型是根据用户活跃程度、交易金额的攻下女，对用户价值进行分群的一种方法
[![2023-11-05-211024.png](https://i.postimg.cc/RFSxftxd/2023-11-05-211024.png)](https://postimg.cc/LqbWFnXg)

**5.2.3.2用户属性分群**

在实际应用中，也可以基于用户属性（自然属性、社会属性）进行分群

比如”北京-男-程序员“的体育用户群体

[![2023-11-05-211220.png](https://i.postimg.cc/QtyYtK8H/2023-11-05-211220.png)](https://postimg.cc/342FfRDT)
将同一群内的用户继续切分以满足个性化需求

#### 5.2.4个性化标签
全面、完整、细致地标签化用户个性化特征

通常把用户的个性化标签称为用户画像

个性化标签生成主要有三种方式：人工打标签、机器打标签、混合打标签（人工+机器）
- 工业界，常用的是混合打标签
- 机器打标签是孤立的，可以结合人工来描述它们之间的关系

**5.2.4.1人工打标签**

人工打标签，即手动打标签，可以打上自然属性标签、社会属性标签、关系属性标签

[![2023-11-05-211615.png](https://i.postimg.cc/RCPJWQxP/2023-11-05-211615.png)](https://postimg.cc/3Wvx6GRp)
**5.2.4.2 机器打标签**

机器打标签也成为自动打标签，是根据用户消费国的文本五年、图片、视频等数据，机器自动学习出用户兴趣、喜好等标签

比如，对文本进行机器打标签，其标签类别有：关键词标签、实体标签、类别标签、聚合标签、主题标签、embedding标签
[![2023-11-05-211839.png](https://i.postimg.cc/nrd35s5H/2023-11-05-211839.png)](https://postimg.cc/m1H31rBq)
**5.2.4.3混合打标签**
- 首先，先人工打分层标签、分群标签等粗粒度标签
- 然后再用机器打细粒度标签

#### 5.2.5用户画像的原则
- 真实性：真实的用户数据，而不是想象的伪需求画像
- 统一性：用户标签与物品标签要统一，双向匹配
- 独立性：语义是相互独立的，有区分度
- 全面性：数据覆盖广--文本、图片、视频等
- 统一性：用户标签与物品标签要统一，双向匹配
#### 5.2.6用户画像的检验
通过业务指标、离线指标、线上指标来评估古用户画像的准确率和覆盖率

线上评估用户画像的指标有画像有点数、画像有点率

- 业务评估指标
  - 抽样检验、业务反馈、A/B测试（点击率、时长）
- 离线评估指标
[![2023-11-05-213515.png](https://i.postimg.cc/J4CJV3Zt/2023-11-05-213515.png)](https://postimg.cc/ThQhnDkX)
(其实，线上评估用户画像依赖A/B测试（也称为A/B实验))[https://zhuanlan.zhihu.com/p/346602966]

### 5.3 用户画像应用
#### 5.3.1 百度的用户画像
[![2023-11-05-214957.png](https://i.postimg.cc/wMPMTpbh/2023-11-05-214957.png)](https://postimg.cc/rR1MQvTm)

#### 5.3.2 微博的用户画像
[![2023-11-05-215114.png](https://i.postimg.cc/Y90t4PF8/2023-11-05-215114.png)](https://postimg.cc/2qssM22B)
#### 5.3.3 今日头条的用户画像
[![2023-11-05-215136.png](https://i.postimg.cc/fknsrTrg/2023-11-05-215136.png)](https://postimg.cc/dZBg7vv8)
### 5.4 用户画像总结
#### 5.4.1 推荐系统的用户画像
推荐系统中，用户画像是给机器看的，不是给人看的
- 主要方法：字符化、向量化
  - 字符画：字符匹配
  - 向量化
    - 关键是：维度、重要度、顺序
[![2023-11-05-215519.png](https://i.postimg.cc/7ZRFsLJ1/2023-11-05-215519.png)](https://postimg.cc/xXKpqnRq)
#### 5.4.2用户画像的本质

1. 用户画像基础
1.1. 用户
1.2. 用户画像的概念
1.3. 用户画像的要素
1.4. 用户画像的关系
1.5. 用户画像的意义
2. 用户画像原理
2.1. 用户画像的方法
2.2. 分层标签
2.3. 分群标签
2.4. 个性化标签
2.5. 用户画像的原则
2.6. 用户画像的评估
3. 用户画像应用
3.1. 百度的用户画像
3.2. 微博的用户画像
3.3. 今日头条的用户画像
4. 用户画像总结
4.1. 推荐系统的用户画像
4.2. 用户画像的本质
4.3. 用户画像的挑战
总结：
结束语：
扩展阅读：
用户画像（User Profile）的本质是用户需求描述，一种刻画用户需求的模型。

用户画像在推荐系统、广告系统、商业分析、数据分析、用户增长、用户研究、产品设计、数据化运营、精准营销、量化风控等领域得到广泛应用。

本文系统地构建了用户画像的知识体系。首先，介绍了用户画像的概念、要素、关系、意义等用户画像的基础知识；接着，介绍了用户画像的方法、分层标签、AARRR 用户分层模型、用户业务分层、分群标签、RFM 用户分群模型、用户属性分群、个性化标签、人工打标签、机器打标签、混合打标签、用户画像的原则、用户画像的评估等用户画像原理；然后，介绍百度的用户画像、微博的用户画像、今日头条的用户画像；最后，指出用户画像在推荐系统中的应用，以及用户画像的本质和用户画像面临的挑战。本文目录如下：

1. 用户画像基础
1.1. 用户
1.2. 用户画像的概念
1.3. 用户画像的要素
1.4. 用户画像的关系
1.5. 用户画像的意义

2. 用户画像原理
2.1. 用户画像的方法
2.2. 分层标签（用户分层）
2.2.1. AARRR 用户分层模型
2.2.2. 用户业务分层
2.3. 分群标签（用户分群）
2.3.1. RFM 用户分群模型
2.3.2. 用户属性分群
2.4. 个性化标签（用户画像）
2.4.1. 人工打标签
2.4.2. 机器打标签
2.4.3. 混合打标签
2.5. 用户画像的原则
2.6. 用户画像的评估

3. 用户画像应用
3.1. 百度的用户画像
3.2. 微博的用户画像
3.3. 今日头条的用户画像

4. 用户画像总结
4.1. 推荐系统的用户画像
4.2. 用户画像的本质
4.3. 用户画像的挑战
接下来，让我一起走进用户画像的世界。


用户画像的基础、原理、方法论（模型）和应用
1. 用户画像基础

用户画像基础的目录


1.1. 用户
要想搞清楚用户画像，首先得搞清楚用户。比如：

你的用户的特征是什么？
怎么描述用户需求？
不同阶段的用户特征又是什么？

用户
关于用户还有用户关键路径、用户旅程（customer journey map）、用户决策过程等等。



1.2. 用户画像的概念
怎样区分User Portrait（用户肖像）、Customer Segment（用户细分）、User Persona（用户角色）、User Profile（用户画像）？


用户画像的概念
用户肖像、用户细分、用户角色和用户画像的对比如下表所示：


用户肖像、用户细分、用户角色和用户画像的对比
高层、产品、开发、市场、运营眼中的用户画像是什么？


高层、产品、开发、市场、运营眼中的用户画像
用户画像是刻画用户需求的模型。

用户画像是一种公共语言，串联互联网商业的高层、产品、开发、市场、运营等，提高沟通效率。


用户画像是刻画用户需求的模型


1.3. 用户画像的要素
用户画像的三要素：人、物、环境。


用户画像的要素


1.4. 用户画像的关系
用户画像描述的是用户与物品的关系。


用户画像的关系


1.5. 用户画像的意义
用户画像广泛应用在推荐系统、广告系统、商业分析、数据分析、用户增长、数据运营、精准营销、量化风控等领域。


用户画像的意义


2. 用户画像原理

用户画像原理的目录


2.1. 用户画像的方法
用户画像的主流方法：用户标签化。


用户画像的方法
标签是用户属性、兴趣、行为等特征的抽象与描述。


标签是用户属性、兴趣、行为等特征的抽象与描述
从分层标签、分群标签到个性化标签，正是一个由粗到细的过程。


分层标签、分群标签、个性化标签


2.2. 分层标签
将总体中各个用户按某种特征分成若干个互不重叠的几部分，每一部分叫做层。

分层标签是指根据分层规则，对用户进行分层而打的标签。

用户分层是指基于分层标签描述用户。


分层标签
2.2.1. AARRR 用户分层模型

举个用户分层的例子：AARRR 用户分层模型。

AARRR 模型是由Acquisition（获取）、Activation（激活）、Retention（留存）、Referral（推荐）、Revenue（变现）等五个部分组成，形成一个用户流量漏斗。

按“从获取的用户到最终推荐的用户的演进路线”进行分层如下：


AARRR 用户分层模型
2.2.2. 用户业务分层

实际应用中，也可以基于具体业务进行用户分层。

比如：今日头条可以按资讯类别把用户分成科技用户、娱乐用户、游戏用户等等。


用户业务分层
将同一层内的用户继续切分以满足精细化需求。


将同一层内的用户继续切分以满足精细化需求
所以还需要继续切分，获取分群标签。



2.3. 分群标签
按照指定规则，将总体中若干个用户合并为组，这样的组称为群。

分群标签是指根据分群规则，对用户进行分群而打的标签。

用户分群是指基于分群标签描述用户。


分群标签
2.3.1. RFM 用户分群模型

举个用户分层的例子：RFM 用户分群模型（Recency、Frequency、Monetary ）。

RFM是根据用户活跃程度、交易金额的贡献，对用户价值进行分群的一种方法。


RFM 用户分群模型
RFM 的 8 个象限分别表示 8 类用户如下：


RFM 的 8 个象限分别表示 8 类用户
2.3.2. 用户属性分群

实际应用中，也可以基于用户属性（自然属性、社会属性）进行分群。

比如“北京-男-程序员”的体育用户群体。


用户属性分群
将同一群内的用户继续切分以满足个性化需求。


将同一群内的用户继续切分以满足个性化需求
所以还需要继续切分，获取个性化标签。



2.4. 个性化标签
全面、完整、细致地标签化用户个性化特征。

通常把用户的个性化标签近似称为用户画像。

个性化标签生成主要三种方式：人工打标签、机器打标签、混合打标签（人工+机器）。


个性化标签
2.4.1. 人工打标签

人工打标签，即手动打标签，可以打上自然属性标签、社会属性标签、关系属性标签等。


人工打标签
2.4.2. 机器打标签

机器打标签，也称自动打标签，是指根据用户消费过的文本、图片、视频等数据，机器自动学习出用户兴趣、喜好等标签。

比如，对文本进行机器打标签，其标签类型有：关键词标签、实体标签、类别标签、聚合标签、主题标签、Embedding标签等。


机器打标签
具体机器怎么打标签，这里不过多展开。感兴趣的可以参考我之前写的相关文章，比如：

“分类标签“的原理和代码实现，可参考：

刘启林：机器学习：二分类、多分类、多标签分类的概述、原理、算法和工具
309 赞同 · 14 评论文章

“实体标签“的原理和代码实现，可参考：

刘启林：中文命名实体识别NER的原理、方法与工具
215 赞同 · 16 评论文章

2.4.3. 混合打标签

首先，先人工打分层标签、分群标签等粗粒度标签；

然后，再用机器打细粒度标签。


混合打标签


2.5. 用户画像的原则
真实性：真实的用户数据，而不是想象的伪需求画像。

统一性：用户标签与物品标签要统一，双向匹配。


用户画像的原则


2.6. 用户画像的评估
通过业务指标、离线指标、线上指标来评估用户画像的准确率和覆盖率。

线上评估用户画像的指标有画像有点数、画像有点率。


用户画像的评估
其实，线上评估用户画像依赖A/B测试（也称A/B试验）。

A/B测试的原理和代码这里也不不进行展开，感兴趣的可以参考我之前写的相关文章：

刘启林：A/B测试(A/B试验)的概述、原理、公式推导、Python实现和应用
1021 赞同 · 70 评论文章



3. 用户画像应用

用户画像应用的目录


3.1. 百度的用户画像
百度的用户画像框架如下：


百度的用户画像框架


3.2. 微博的用户画像
微博的用户画像框架如下：


微博的用户画像框架


3.3. 今日头条的用户画像
今日头条的用户画像框架如下：


今日头条的用户画像框架
百度、微博、今日头条的用户画像对比如下：


百度、微博、今日头条的用户画像对比


4. 用户画像总结

用户画像总结的目录


4.1. 推荐系统的用户画像
推荐系统中，用户画像是给机器看得，不是给人看的。


推荐系统的用户画像
用户画像在推荐系统中的应用：

召回阶段：用户画像用于物品过滤
排序阶段：用户画像用于物品排序

用户画像在推荐系统中的应用


4.2. 用户画像的本质
用户画像是刻画用户需求的模型，所以用户画像的本质是用户需求。
#### 5.4.3用户画像的挑战
- 用户画像其实是用过去预测现在，但用户需求变化很快
- 用户画像若只刻画自己喜欢的东西，容易陷入信息茧房。

## 6 推荐系统
https://github.com/wangshusen/RecommenderSystem

推荐系统中的A/Btest测试

"C:\Users\23665\Desktop\研一上课资料\111\推荐系统\01_Basics_03.pdf"
