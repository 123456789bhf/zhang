1. CART回归树是怎么实现的？（贝壳）

CART回归树的实现包含两个步骤：

1）决策树生成：基于训练数据生成决策树、生成的决策树要尽量大

2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

这部分的知识，可以看一下《统计学习方法》一书。

2. CART分类树和ID3以及C4.5有什么区别（贝壳）

1）首先是决策规则的区别，CART分类树使用基尼系数、ID3使用的是信息增益，而C4.5使用的是信息增益比。

2）ID3和C4.5可以是多叉树，但是CART分类树只能是二叉树（这是我当时主要回答的点）

3. 树集成模型有哪几种实现方式？（贝壳）boosting和bagging的区别是什么？（知乎、阿里）

树集成模型主要有两种实现方式，分别是Bagging和Boosting。二者的区别主要有以下四点：

1）样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的.

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化.而权值是根据上一轮的分类结果进行调整.

2）样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大.

3）预测函数：

Bagging：所有预测函数的权重相等.

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重.

4）并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果.

4. 随机森林的随机体现在哪些方面（贝壳、阿里）

1） 随机森林的随机主要体现在两个方面：一个是建立每棵树时所选择的特征是随机选择的；

2) 二是生成每棵树的样本也是通过有放回抽样产生的。

3. AdaBoost是如何改变样本权重，GBDT分类树的基模型是？（贝壳）AdaBoost改变样本权重：增加分类错误的样本的权重，减小分类正确的样本的权重。

最后一个问题是我在面试之前没有了解到的，GBDT无论做分类还是回归问题，使用的都是CART回归树。

4. gbdt,xgboost,lgbm的区别(百度、滴滴、阿里，头条)首先来看GBDT和Xgboost，二者的区别如下：

1）传统 GBDT 以 CART 作为基分类器，xgboost 还支持线性分类器，这个时候 xgboost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归(分类问题)或者线性回归(回归问题)。

2）传统 GBDT 在优化时只用到一阶导数信息，xgboost 则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便 一下，xgboost 工具支持自定义代价函数，只要函数可一阶和二阶求导。

3）xgboost 在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的 score 的 L2 模的平方和。从 Bias-variance tradeoff 角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是 xgboost 优于传统GBDT 的一个特性。

4）Shrinkage(缩减)，相当于学习速率(xgboost 中的eta)。xgboost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削 弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把 eta 设置得小一点，然后迭代次数设置得大一点。(补充:传统 GBDT 的实现 也有学习速率)

5）列抽样(column subsampling)。xgboost 借鉴了随机森林的做法，支 持列抽样，不仅能降低过拟合，还能减少计算，这也是 xgboost 异于传 统 gbdt 的一个特性。

6）对缺失值的处理。对于特征的值有缺失的样本，xgboost 可以自动学习 出它的分裂方向。

7）xgboost 工具支持并行。boosting 不是一种串行的结构吗?怎么并行的? 注意 xgboost 的并行不是 tree 粒度的并行，xgboost 也是一次迭代完才能进行下一次迭代的(第 t 次迭代的代价函数里包含了前面 t-1 次迭代 的预测值)。xgboost 的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序(因为要确定最佳分割点)，xgboost在训练之前，预先对数据进行了排序，然后保存为 block 结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每 个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

8）可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以 xgboost 还 出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

5. 再来看Xgboost和LightGBM，二者的区别如下：

1）由于在决策树在每一次选择节点特征的过程中，要遍历所有的属性的所有取 值并选择一个较好的。XGBoost 使用的是近似算法算法，先对特征值进行排序 Pre-sort，然后根据二阶梯度进行分桶，能够更精确的找到数据分隔点;但是 复杂度较高。LightGBM 使用的是 histogram 算法，这种只需要将数据分割成不同的段即可，不需要进行预先的排序。占用的内存更低，数据分隔的复杂度更低。

2）决策树生长策略，我们刚才介绍过了，XGBoost采用的是 Level-wise 的树 生长策略，LightGBM 采用的是 leaf-wise 的生长策略。

3）并行策略对比，XGBoost 的并行主要集中在特征并行上，而 LightGBM 的并 行策略分特征并行，数据并行以及投票并行。

5. bagging为什么能减小方差？（知乎）

这个当时也没有答上来，可以参考一下博客：https://blog.csdn.net/shenxiaoming77/article/details/53894973

树模型相关的题目以上就差不多了。接下来整理一些最近群友提出的问题，我觉得有一些可能作为面试题，有一些是准备校招过程中的经验：

6. 关于AUC的另一种解释：是挑选一个正样本和一个负样本，正样本排在负样本前面的概率？如何理解？

我们都知道AUC是ROC曲线下方的面积，ROC曲线的横轴是真正例率，纵轴是假正例率。我们可以按照如下的方式理解一下：首先偷换一下概念，意思还是一样的，任意给定一个负样本，所有正样本的score中有多大比例是大于该负类样本的score？那么对每个负样本来说，有多少的正样本的score比它的score大呢？是不是就是当结果按照score排序，阈值恰好为该负样本score时的真正例率TPR？理解到这一层，二者等价的关系也就豁然开朗了。ROC曲线下的面积或者说AUC的值 与 测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score是等价的。
## ID3，ID4.5,CART之间的区别以及优缺点
### ID3
 **缺点**
- ID3 没有剪枝策略，容易过拟合；

- 信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1；

- 只能用于处理离散分布的特征；
- 没有考虑缺失值。
### ID4.5
**C4.5 算法最大的特点**是克服了 ID3 对特征数目的偏重这一缺点，引入信息增益率来作为分类标准。

C4.5 相对于 ID3 的缺点对应有以下改进方式：

- 引入悲观剪枝策略进行后剪枝；
- 引入信息增益率作为划分标准；
- 将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点；

**对于缺失值的处理可以分为两个子问题：**
  
问题一：在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）

问题二：选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里）

针对问题一，C4.5 的做法是：对于具有缺失值特征，用没有缺失的样本子集所占比重来折算；

针对问题二，C4.5 的做法是：将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。

**2.2 划分标准**

这里需要注意，信息增益率对可取值较少的特征有所偏好（分母越小，整体越大），因此 C4.5 并不是直接用增益率最大的特征进行划分，而是使用一个启发式方法：先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的。

**2.3 剪枝策略**

为什么要剪枝：过拟合的树在泛化能力的表现非常差。

**2.3.1 预剪枝**

在节点划分前来确定是否继续增长，及早停止增长的主要方法有：

- 节点内数据样本低于某一阈值；
- 所有节点特征都已分裂；
- 节点划分前准确率比划分后准确率高。
- 
预剪枝不仅可以降低过拟合的风险而且还可以减少训练时间，但另一方面它是基于“贪心”策略，会带来欠拟合风险。

**2.3.2 后剪枝**

在已经生成的决策树上进行剪枝，从而得到简化版的剪枝决策树。

C4.5 采用的悲观剪枝方法，用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率。

后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但同时其训练时间会大的多。

**2.4 缺点**

剪枝策略可以再优化；

C4.5 用的是多叉树，用二叉树效率更高；

C4.5 只能用于分类；

C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；

C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。

### CART

ID3 和 C4.5 虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但是其生成的决策树分支、规模都比较大，CART 算法的二分法可以简化决策树的规模，提高生成决策树的效率。

**3.1 思想**

CART 包含的基本过程有分裂，剪枝和树选择。

分裂：分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去；

剪枝：采用代价复杂度剪枝，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树；

树选择：用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）。

CART 在 C4.5 的基础上进行了很多提升。
**CART的有点**
- C4.5 为多叉树，运算速度慢，CART 为二叉树，运算速度快；
- C4.5 只能分类，CART 既可以分类也可以回归；
- CART 使用 Gini 系数作为变量的不纯度量，减少了大量的对数运算；
- CART 采用代理测试来估计缺失值，而 C4.5 以不同概率划分到不同节点中；
- CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法。
- 
**3.2 划分标准**
  
熵模型拥有大量耗时的对数运算，基尼指数在简化模型的同时还保留了熵模型的优点。基尼指数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（率）正好相反。

**3.3 缺失值处理**

上文说到，模型对于缺失值的处理会分为两个子问题：

如何在特征值缺失的情况下进行划分特征的选择？

选定该划分特征，模型对于缺失该特征值的样本该进行怎样处理？

对于问题 1，CART 一开始严格要求分裂特征评估时只能使用在该特征上没有缺失值的那部分数据，在后续版本中，CART 算法使用了一种惩罚机制来抑制提升值，从而反映出缺失值的影响（例如，如果一个特征在节点的 20% 的记录是缺失的，那么这个特征就会减少 20% 或者其他数值）。

对于问题 2，CART 算法的机制是为树的每个节点都找到代理分裂器，无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理（即代理就是代替缺失值特征作为划分特征的特征），当 CART 树中遇到缺失值时，这个实例划分到左边还是右边是决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点。代理分裂器可以确保无缺失训练数据上得到的树可以用来处理包含确实值的新数据。

3.4 剪枝策略
采用一种“基于代价复杂度的剪枝”方法进行后剪枝，这种方法会生成一系列树，每个树都是通过将前面的树的某个或某些子树替换成一个叶节点而得到的，这一系列树中的最后一棵树仅含一个用来预测类别的叶节点。然后用一种成本复杂度的度量准则来判断哪棵子树应该被一个预测类别值的叶节点所代替。这种方法需要使用一个单独的测试数据集来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树。

## 个体与集中
集成学习：通过构建并结合多个学习起来完成学习任务，有时也被称为多分类器系统

集成学习分类：
- 同质集成学习：子学习器的学习算法相同，此时子学习器称为基学习器，相应的学习算法为基学习算法
- 异质集成学习：子学习器的学习算法不同，例如，同时包含决策树，神经网络

根据个体生成器的生成方式不同，目前的集成学习分为两大类
- 子学习器之间存在强相互依赖关系，必须串行生成的序列化方法，代表：boosting
- 子学习器之间不存在强相互关系，可同时生成的并行化方法，代表：随机森林，bagging

### boosting（串行）

- 先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器;如此重复进行，直至基学习器数目达到事先指定的值 T ， 最终将这 T 个基学习器进行加权结合.  
  
- Boosting 族算法最著名的代表是 AdaBoost [Freund and Schapire, 1997] ,
其描述如图 8.3 所示，其中 Yi E {-1,+1}, f 是真实函数.

- Boosting 算法要求基学习器能对特定的数据分布进行学习，这可通过
-  **"重赋权法"**  (re-weighting)实施，即在训练过程的每一轮中，根据样本分布为每个
训练样本重新赋予一个权重。
- 对无法接受带权样本的基学习算法，则可通过 **"重采样法"** (re-sampling)来处理，即在每一轮学习中，根据样本分布对训练集重新进行采样，再用重采样而得的样本集对基学习器进行训练.

一般而言，这两种做法没有显著的优劣差别.需注意的是， Boosting 算法在训练的每一轮都要检查当前生成的基学习器是否满足基本条件(例如罔 8.3 的第 5 行，检查当前基分类器是否是比随机猜测好) ，一旦条件不满足，则当前基学习器即被抛弃，且学习过程停止.在此种情形下，初始设置的学习轮数 T 也许遥远未达到，可能导致最终集成中只包含很少的基学习器而性能不佳.若采用"重采样法"，则可获得"重启动"机会以避免训练过程过早停止 [Kohavi and Wolpert, 1996],即在抛弃不满足条件的当前基学习器之后，可根据当前分布重新对训练样本进行采样，再基于新的采样结果重新训练出基学习器，从而使得学习过程可以持续到预设的 T 轮完成.
### Bagging与随机森林（并行）
#### Bagging
##### 流程

Bagging是并行集成学习的最著名的代表，给定包含m个样本的数据集，我们先随机抽取一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样的时候该样本仍有可能被选中，这样，经过m词采样操作，我们得到含有m个样本的采样机，初始训练集中有的样本在采样激励多次出现，有的则为出现。照这样，我们可采样出T个含有 m个训练样本的采样机，然后基于每个采样机训练出一个学习器，再将这些基学习器进行结合。在对预测输出进行结合是，Bagging通常对分类任务使用**简单投票法（多数表决）**，对回归任务使用**简单平均法**。若分类预测时候出现两个类收到同样票数的情形，那么最简单的做法是随机选择一个，也可进一步考察学习其投票的置信度来确定最终胜利者

##### 优点
1. Bagging的时间复杂度与基学习器的同阶，说明算法非常高效
2. adaBoost算法只能用于二分类问题，Bagging 方法可以用于多分类以及回归问题
3. 由于基学习器只用了原始训练数据集的63.2%的样本，剩下约36.8%的样本可用来估计泛化性能做“包外估计”
4. 包外样本还有许多其他用途.例如当基学习器是决策树时，可使用包外样本来辅助剪枝?或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理;当基学马若是是神经网络时?可使用包外样本来辅助早期停止以减小过拟合风险.
#### 随机森林（RF）
随机森林(Random Forest，简称 RF) [Breiman, 2001a] 是 Baggi吨的一个
扩展变体.盯在以决策树为基学习器构建 Bagging 集成的基础上，进一步在
决策树的训练过程中引入了随机属性选择.具体来说，传统决策树在选择划分
属性时是在当前结点的属性集合(假定有 d 个属性)中选择一个最优属性;而在
RF 中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含 k
个属性的子集，然后再从这个子集 中选择一个最优属性用 于划分. 这里 的参数
k 控制了 随机性的引入程度 ;若令 k = d ， 则基决策树的构建与 传统决策树相同;
若令 k = 1 ， 则是随机选择一个属性用 于划分 ; 一般情况下，推荐值 k = log2 d
[Breiman, 2001叫.

####### 随机森林的优点
1. 简单
2. 容易实现
3. 计算开销小
4. 随机森林的训练效率常优于 Bagging，因为在个体决策树的构建过程中 ， Bagging
使用 的是 "确定型" 决策树?在选择划分属性时要对结点的所有属性进行考察 ，
而随机森林使用的" 随机型"决策树则只需考察-个属性子集

#### 组合策略

##### 学习器组合的优点
1. 由于学习任务的假设空间往往很大，可能有多个假设在训 练集上
达到同等性能 ，此时若使用单学 习 器可能因误选而导致泛化性能不佳，结合多
个学习器则会减小这一风险?
2. 从计算的方面来看，学习算法往往会陷入局
部极小?有的局部极小 点所对应 的泛化性能可能很糟糕 ， 而通过多次运行之后
进行结合 ， 可降低陷入糟糕局部极小点的风险
3. 从表示的方面来看，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中 ，此时若使用单学习器则肯定无效，而通过结合多个学习器 ， 由于相应的假设空间有所扩大，
有可能学得更好的近似

###### 常见的组合策略
1. 简单平均法
2. 加权平均法
   - 显然，简单平均法是加权平均法的特例。
   - 加权平均法的权重一般是从训练数据中学习而得，现实任务中的训练样本
通常不充分或存在噪声，这将使得学出的权重不完全可靠.尤其是对规模比较
大的集成来说?要学习的权重比较多，较容易导致过拟合.因此，实验和应用均
显示出，力日权平均法未必一起优于简单平均法 [Xu et al., 1992; Ho et al., 1994;
Kittler et al., 1998]. 一般而言，在个体学习器性能相差较大时宜使用加权平均
法?而在个体学习器性能相近时宜使用简单平均法.
3. 投票法
   - 绝对多数表决法：即若某标记得票过半数，则预测为该标记;否则拒绝预测.
   - 相对多数投票法：即预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个
   - 加权投票法：与加权平均法类似 
  
4.学习法

即通过另一个学习器进行结合， **Stacking** [Wolpert, 1992; Brei皿础， 1996b] 是学习法的典型代表.这里我们把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器(meta-learn)

Stacking 先从初始数据集训练出初级学习器，然后"生成"一个新数据集
用于训练次级学习器.在这个新数据集中，**初级学习器的输出被当作样例输入
特征，而初始样本的标记（输出）仍被当作样例标记（输出）** 这里我们假定初级学习器使用不同学习算法产生，即初级集成是异质的.

在训练阶段，次级训练集是利用初级学习器产生的，若直接用初级学习器
的输出结果来产生次级训练集，则过拟合风险会比较大;因此，一般是通过使用交
叉验证或留一法这样的方式 ，用训练初级学习器未使用的样本来产生次级学习
器的训练样本.以 k 折交叉验证为例，初始训练集 D 被随机划分为 k 个大小相
似的集合 D1 ， D2 ， … ， Dk' 令 Dj 和 Dj = D \ Dj 分别表示第 j 折的测试集和
训练集给定 T 个初级学习算法，初级学习器时j) 通过在马上使用第 t 个学
习算法而得.对 Dj 中每个样本矶，令 zzt=hjJ)(z山则由 Xi 所产生的次级训
练样例的示例部分为均= (Zi1; Zi2; . . . ; ZiT) ，标记部分为 Yi. 于是，在整个交叉
验证过程结束后，从这 T 个初级学习器产生的次级训练集是 D'={(zi,yi},i=1,...m
然后 D' 将用于训练次级学习器。这样总共进行k词，也就是整个训练数据集都有用到。。

次级学习器的输入属性表示和次级学习算法对 Stacking 集成的泛化性能
有很大影响.有研究表明，将初级学习器的输出类概率作为次级学习器的输入
属性，用多响应线性回归 (Multi-response Linear Regression，简称 MLR) 作为
次级学习算法效果较好 [Ting and Witten, 1999]，在 MLR 中使用不同的属性
集更佳 [Seewald ， 2002]

贝叶斯模型平均 (Bayes Model Averaging，简称 BMA)基于后验概率来为
不同模型赋予权重 7 可视为加权平均法的一种特殊实现. [Clarke, 2003] 对
Stacking 和 BMA 进行了比较.理论上来说?若数据生成模型怡在当前考虑的
模型中，且数据噪声很少，则 BMA 不差于 Stacking; 然而，在现实应用中无法
确保数据生成模型一定在当前考虑的模型中，甚至可能难以用当前考虑的模型
来进行近似，因此， Stacking 通常优于 BMA，因为其鲁棒性比 BMA 更好，而且
BMA 对模型近似误差非常敏感。

##### 多样性度量
多样性度量：也就是估计集成中个体学习器的多样性，即估算个体学习器的多样化程度，典型做法是考虑个体学习器的两两相似/不相似性：
- 不合度量：越大多样性越大
- 相关系数：相关系数值域{-1，1],如果不相关，那么为0，如果正相关，那么为正，否则为负数
- Q-统计量:符号于相关系数相同，但绝对值小于等于相关系数绝对值
- k-统计量：若分类器 hi 与 hj 在 D 上完全一致，则 κ= 1; 若它们仅是偶然达成一致，则 κ= o，κ 通常为非负值，仅在 hi 与 hj 达成一致的概率甚主低于偶然
性的情况下取负值.

##### 多样性增强
在集成学习中需有效地生成多样性大的个体学习器 . 与简单地直接用初始
数据训练出个体学习器相比，如何增强多样性呢?一般思路是在学习过程中引
入随机性，常见做法主要是对数据样本、 输入属性、输出表示 、 算法参数进行
扰动
- 数据样本扰动

给定初始数据集， 可从中产生出不同的数据子集， 再利用不 同 的数据子集
训练出不同的个体学习器.数据样本扰动通常是基于采样法， 例如在 Bagging
中使用 自 助采样，在 AdaBoost 中使用序列采样. 此类做法简单高效，使用最
广.对很多常见的基学习器，例如决策树、 神经网络等 ， 训练样本稍加变化就会
导致学习器有显著变动，数据样本扰动法对这样的"不稳定基学习器"很有效;
然而，有一些基学习器对数据样本的扰动不敏感，例如线性学习器、支持向量
机 、 朴素贝叶斯、 k 近邻学习器等 ， 这样的基学习器称为稳定基学习器 (stable
base learner)，对此类基学习器进行集成往往需使用输入属性扰动等其他机制 .
- 输入属性扰动

训练样本通常由一组属性描述，不同的"子空间" (subspace，即属性子
集)提供了观察数据的不同视角.显然 7 从不同子空间训练出的个体学习器必然
有所不同.著名的随机子空间 (random subspace)算法 [Ho， 1998] 就依赖于输入
属性扰动，该算法从初始属性集中抽取出若干个属性子集，再基于每个属性子
集训练一个基学习器，算法描述如图 8.11 所示.对包含大量冗余属性的数据，
在子空间中训练个体学习器不仅能产生多样性大的个体，还会因属性数的减少
而大幅节省时间开销，同时?由于冗余属性多，减少一些属性后训练出的个体学
习器也不至于太差.若数据只包含少量属性，或者冗余属性很少?则不宜使用输
入属性扰动法.

- 输出表示扰动

此类做法的基本思路是对输出表示进行操纵以增强多样性.可对训练样本
的类标记稍作变动，如"翻转法" (Flipping Output) [Breiman, 2000] 随机改变
一些训练样本的标记;也可对输出表示进行转化，如"输出调制法" (Output
Smearing) [Breiman, 2000] 将分类输出转化为回归输出后构建个体学习器;
还可将原任务拆解为多个可同时求解的子任务，如 ECOC 法 [Dietterich and
Bakiri, 1995] 利用纠错输出码将多分类任务拆解为一系列二分类任务来训练基
学习器.

- 算法参数扰动

基学习算法一般都有参数需进行设置，例如神经网络的隐层神经元数、初
始连接权值等?通过随机设置不同的参数?往往可产生差别较大的个体学习器.例如"负相关法" (Negative Correlation) [Liu and Yao, 1999] 思式地通过正则
化项来强制个体神经网络使用不同的参数.对参数较少的算法，可通过将其学
习过程中某些环节用其他类似方式代替?从而达到扰动的目的，例如可将决策
树使用的属性选择机制替换成其他的属性选择机制.值得指出的是，使用单一
学习器时通常需使用交叉验证等方法来确定参数值，这事实上已使用了不同参
数训练出多个学习器，只不过最终仅选择其中一个学习器进行使用，而集成学
习则相当于把这些学习器都利用起来;由此也可看出，集成学习技术的实际计
算开销并不比使用单一学习器大很多.
  






