### 1.LightGBD优势
1. 更快的训练速度和更高的效率：LightGBM使用基于直方图的算法。
2. 更低的内存占用：使用离散的箱子(bins)保存并替换连续值导致更少的内存占用。
3. 更高的准确率(相比于其他任何提升算法)：它通过leaf-wise分裂方法产生比level-wise分裂方法更复杂的树，这就是实现更高准确率的主要因素。然而，它有时候或导致过拟合，但是我们可以通过设置|max-depth|参数来防止过拟合的发生。
4. 大数据处理能力：相比于XGBoost，由于它在训练时间上的缩减，它同样能够具有处理大数据的能力。
5. 支持并行学习。
### 2.BatchNormalization的作用，为什么要在后面加伽马和贝塔，不加可以吗
神经网络在训练的时候随着网络层数的加深,激活函数的输入值的整体分布逐渐往激活函的取值区间上下限靠近,从而导致在反向传播时低层的神经网络的梯度消失。
而Batch Normalization的作用是把一个batch内的所有数据，通过规范化的手段,将越来越偏的分布拉回到标准化的分布,使得激活函数的输入值落在激活函数对
输入比较敏感的区域,从而使梯度变大,加快学习收敛速度,避免梯度消失的问题。加入缩放平移变量γ和β的原因是： 不一定每次都是标准正态分布，也许需要偏移
或者拉伸。直白的说就是为了保证每一次数据经过归一化后还保留原有学习来的特征，同时又能完成归一化操作，加速训练。 这两个参数是用来学习的参数。1.保
留网络各层在训练过程中的学习成果；2. 保证激活单元的非线性表达能力；3. 使批归一化具有自我关闭能力。
### 3.Relu比Sigmoid的效果好在哪里?
1. 可以防止过拟合
2. relu激活函数的单侧抑制提供了模型的稀疏性表达能力
### 4.神经网络激活函数？
### 5. 如何解决梯度消失梯度爆炸问题？
- 梯度消失
1非饱和的激活函数（如 ReLU）
2批量规范化（Batch Normalization）
- 梯度爆炸哦
1. 梯度截断（Gradient Clipping）
2. resnet网络
### 6. 1*1的卷积核
1. 实现跨通道的交互和信息整合,
2. 实现卷积核通道数的降维和升维,
3. 与全连接层等价
4. 控制模型的复杂度
### 7. 用过哪些 Optimizer，效果如何
- 优化算法是不断迭代模型参数以降低模型损失函数的值
- 1）SGD；2）Momentum；3）Nesterov；4）Adagrad；5）Adadelta；6）RMSprop；7）Adam；8）Adamax；9）Nadam。

1. 对于稀疏数据，尽量使用学习率可自适应的算法，不用手动调节，而且最好采用默认参数。
2. SGD通常训练时间最长，但是在好的初始化和学习率调度方案下，结果往往更可靠。但SGD容易困在鞍点，这个缺点也不能忽略。
3. 如果在意收敛的速度，并且需要训练比较深比较复杂的网络时，推荐使用学习率自适应的优化方法。
4. Adagrad，Adadelta和RMSprop是比较相近的算法，表现都差不多。
5. 在能使用带动量的RMSprop或者Adam的地方，使用Nadam往往能取得更好的效果。
