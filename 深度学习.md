# 深度学习
# 深度学习 - 八股
## 4. 多层感知机
-  多层感知机（MLP）:如果只有一个隐藏层，如果节点数（神经元个数）足够多，并且参数合适，也是可以拟合复杂函数的，但是这样一般比较困难，采用神的神经网络可以更加简单的拟合函数关系
-  relu激活函数的好处：
  - -  可以减轻梯度消失的问题
  - -  求导表现好：要么让梯度消失，要么让梯度为0
- tanh激活函数在0处接近于线性变换
- 每一层神经元个数常为2的次方，这样计算更加高效
- 过拟合：模型复杂，数据参数lamda越大简单
- 欠拟合：模型简单，数据复杂
- 如果泛化误差与测试误差之间差距小，那么是欠拟合，差距大，那么是过拟合
- 训练误差与泛化误差都大，可能是欠拟合，训练误差非常小，泛化误差非常大，那么可能是过拟合。
- 多层感知机的假设：训练数据集和验证集上的数据是独立同分布的。有的时候，轻微的违背独立同分布的假设结果可能运行的很好。
- K-折交叉验证：当数据量较少的时候，将数据集分为k份，每次用一份做验证集，其他k-1份做训练集，总共进行k次验证，最后通过k此实验的结果平均来估计训练和泛化误差。
- 防止过拟合的方法
- - 权重衰减（正则化）:参数lamda较小时，对参数的约束越小，，对参数约束越大。
- - 通常网络输出层的偏执b不需要正则化，因为没有激活函数，再有激活函数的时候可能需要偏执b.
  - 增加训练数据
  - 使用适当复杂度的模型
  - 暂退发（丢弃发，dropout）:在隐藏层的输出上增加一个噪声，类似于一个正则表达式
    - 通过在模型的输出上随机置0来控制模型的复杂度
    - 只在训练的时候使用
    - 通常放在隐藏层的输出上
    - 丢弃发是控制模型复杂度的超参数（概率p是自己定的）
    - 理论：具有噪声的训练等价于正则化，也就说明了要求函数光滑与要求函数对输入的噪声具有适应性之间有联系
    - 提出的原因：对模型加入噪声会增加输入 - 输出映射的平滑性
