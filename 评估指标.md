# 评估指标

机器学习的评价指标有精度、精确率、召回率、P-R曲线、F1 值、TPR、FPR、ROC、AUC等指标，还有在生物领域常用的敏感性、特异性等指标。

## 基础

在分类任务中，各指标的计算基础都来自于对正负样本的分类结果，用混淆矩阵表示，如 **图1** 所示：

<center><img src="https://raw.githubusercontent.com/w5688414/paddleImage/main/metrics_img/confusion_metric.png" width="500" hegiht="" ></center>
<center><br>图1 混淆矩阵 </br></center>

## 精度


$$Accuracy=\frac{TP+TN}{TP+FN+FP+TN}$$

即所有分类正确的样本占全部样本的比例。

## 精确率
精准率又叫做：Precision、查准率

$$Precision=\frac{TP}{TP+FP}$$

即预测是正例的结果中，确实是正例的比例。

## 召回率

召回率又叫：Recall、查全率

$$Recall=\frac{TP}{TP+FN}$$

即所有正例的样本中，被找出的比例

召回率的应用场景：比如望月违约率为例，相对好拥护，我们更关心坏用户，不能错放过任何一个坏用户。因为如果我们果果的坏用户被当成好用户，这样后续发生的违约金额会远远超过好用户仓换的借款利息金额，造成严重的损失。召回率越高，代表实际坏用户被检测出来的概率越高，它的含义是：**宁可错杀一千，也不放过过一个**

### 精准率与召回率缺点
- 因为很多都是分类问题，所以在于预测的时候我们需要设置一个阈值，如果输出大于这个阈值，那么将其分为正lei，否则，父类，这样对于不同的阈值对应着不同的精准率与召回率，寻找最佳的阈值比较麻烦。
- 什么是最好的阈值点：我们对这两指标的要求：希望精准率与召回率越高越好，但实际上这是相矛盾的，无法做到双高。在p-R途中表明如果一个非常高，另一个一定非常低。选取合适的阈值点要根据实际的要求，比如我们想要搞得召回率就要牺牲一些精准率
- 对于样本不平衡的时候不合适


## P-R曲线

P-R曲线又叫做：PRC

<center><img src="https://raw.githubusercontent.com/w5688414/paddleImage/main/metrics_img/PRC.png" width="500" hegiht="" ></center>
<center><br>图2 PRC曲线图</br></center>


- 其中图像如何绘制
  
有的朋友疑惑：这条曲线是根据什么变化的？为什么是这个形状的曲线？其实这要从排序型模型说起。拿逻辑回归举例，逻辑回归的输出是一个0到1之间的概率数字，因此，如果我们想要根据这个概率判断用户好坏的话，我们就必须定义一个阈值。通常来讲，逻辑回归的概率越大说明越接近1，也就可以说他是坏用户的可能性更大。比如，我们定义了阈值为0.5，即概率小于0.5的我们都认为是好用户，而大于0.5都认为是坏用户。因此，对于阈值为0.5的情况下，我们可以得到相应的一对查准率和查全率。

但问题是：这个阈值是我们随便定义的，我们并不知道这个阈值是否符合我们的要求。因此，为了找到一个最合适的阈值满足我们的要求，我们就必须遍历0到1之间所有的阈值，而每个阈值下都对应着一对查准率和查全率，从而我们就得到了这条曲线。

- 进一步分析

<center><img src="https://pic1.zhimg.com/v2-fa55ae6a09aefc608880ccc3f4ce2140_r.jpg" width="500" hegiht="" ></center>
<center><br>图2 PRC曲线图</br></center>

- 从上图不难发现，precision与Recall的折中(trade off)，曲线越靠近右上角性能越好，曲线下的面积叫AP分数，能在一定程度上反应模型的精确率和召回率都很高的比例。但这个值不方便计算，综合考虑精度与召回率一般使用F1函数或者AUC值（因为ROC曲线很容易画，ROC曲线下的面积也比较容易计算）.
- 分析方法
  - 先看平滑不平滑，在看谁上谁下（同一测试集上），一般来说，上面的比下面的好（红线比黑线好），这样决定了选哪个**模型**；
  - F1（计算公式略）当P和R接近就也越大，一般会画连接(0,0)和(1,1)的线，线和PRC重合的地方的就是F1,F1考虑了精准率与召回率，让两者进行了一个平衡，都比较高。根据F1决定选哪个**阈值。**


## F1 值

$$F1=\frac{2 * P * R}{P + R}$$

### 优点
F1考虑了精准率与召回率，让两者进行了一个平衡，都比较高
### 缺点
阈值需要自己选取

## TPR

真正例率，与召回率相同

$$TPR=\frac{TP}{TP+FN}$$

## FPR

假正例率

$$FPR=\frac{FP}{TN+FP}$$

## ROC

ROC曲线中的主要两个指标就是真正率和假正率，上面也解释了这么选择的好处所在。其中横坐标为假正率（FPR），纵坐标为真正率（TPR），下面就是一个标准的ROC曲线图。

<center><img src="https://pic1.zhimg.com/v2-e0aaf265791ae690b514151454b63d20_r.jpg" width="500" hegiht="" ></center>
<center><br>图2 PRC曲线图</br></center>

**ROC曲线阈值问题**

与前面的P-R曲线类似，ROC曲线也是通过遍历所有阈值来绘制整条曲线的。如果我们不断的遍历所有阈值，预测的正样本和负样本是在不断变化的，相应的在ROC曲线图中也会沿着曲线滑动。

<center><img src="https://pic2.zhimg.com/v2-296b158ebb205a2b90d05f5d2074bbe9_b.webp" width="500" hegiht="" ></center>
<center><br>图2 PRC曲线图</br></center>

**如何判断ROC曲线的好坏**

改变阈值只是不断地改变预测的正负样本数，即TPR和FPR，但是曲线本身是不会变的。那么如何判断一个模型的ROC曲线是好的呢？这个还是要回归到我们的目的：FPR表示模型虚报的响应程度，而TPR表示模型预测响应的覆盖程度。我们所希望的当然是：虚报的越少越好，覆盖的越多越好。所以总结一下就是TPR越高，同时FPR越低（即ROC曲线越陡），那么**模型的性能**就越好。参考如下动态图进行理解

<center><img src="https://pic2.zhimg.com/v2-4ea5d54254cf5f969095d5e12703974d_b.webp" width="500" hegiht="" ></center>
<center><br>图2 PRC曲线图</br></center>



## AUC

Area Under ROC Curve，ROC曲线下的面积：

<center><img src="https://raw.githubusercontent.com/w5688414/paddleImage/main/metrics_img/AUC.png" width="500" hegiht="" ></center>
<center><br>图3 ROC曲线图</br></center>

### 进一步分析
1. 什么是AUC

AUC是而分类模型的评价指标，对于二分类模型，还有其他的评价指标，比如logloss,accuracy,precision,recall,但是较常用的是AUC,Logloss,因为很对机器学习的模型对分类问题的预测结果都是概率，如果要计算accuracy,需要将概率转换为类别，这就需要手动设置一个阈值，如果一个样本的预测概率高于这个阈值，那么就把这个样本放进这个里面，低于这个与之，放进零一个类别里面。使用AUC或者logloaa就可以避免将概率转换为类别。

2. AUC的具体定义
   
即AUC是指随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值比分类器输出该负样本为正的那个概率值要大的可能性。所以AUC反应的是分类器对样本的排序能力。

3. AUC的优点

根据这个解释，如果我们完全随机的对样本分类，那么AUC应该接近0.5。（所以一般训练出的模型，AUC>0.5,如果AUC=0.5，这个分类器等于没有效果，效果与完全随机一样，如果AUC<0.5，则可能是标签标注错误等情况造成）；

另外值得注意的是，AUC的计算方法同时考虑了学习器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器做出合理的评价。AUC对样本类别是否均衡并不敏感，这也是不均衡样本通常用AUC评价学习器性能的一个原因。

4. AUC的计算方法

AUC的计算方法有多种，从物理意义角度理解，AUC计算的是ROC曲线下的面积：
见链接：https://zhuanlan.zhihu.com/p/43405406

5.AUC的一半判断标准

参考链接：https://zhuanlan.zhihu.com/p/46714763

0.5 - 0.7：效果较低，但用于预测股票已经很不错了

0.7 - 0.85：效果一般

0.85 - 0.95：效果很好

0.95 - 1：效果非常好，但一般不太可能




## 敏感性

敏感性或者灵敏度（Sensitivity，也称为真阳性率）是指实际为阳性的样本中，判断为阳性的比例（例如真正有生病的人中，被医院判断为有生病者的比例），计算方式是真阳性除以真阳性+假阴性（实际为阳性，但判断为阴性）的比值（能将实际患病的病例正确地判断为患病的能力，即患者被判为阳性的概率）。公式如下：

$$sensitivity =\frac{TP}{TP + FN}$$

即有病（阳性）人群中，检测出阳性的几率。（检测出确实有病的能力）

## 特异性

特异性或特异度（Specificity，也称为真阴性率）是指实际为阴性的样本中，判断为阴性的比例（例如真正未生病的人中，被医院判断为未生病者的比例），计算方式是真阴性除以真阴性+假阳性（实际为阴性，但判断为阳性）的比值（能正确判断实际未患病的病例的能力，即试验结果为阴性的比例）。公式如下：

$$specificity =\frac{TN}{TN + FP}$$

即无病（阴性）人群中，检测出阴性的几率。（检测出确实没病的能力）
## 具体的应用场景
1. 地震的预测对于地震的预测，我们希望的是RECALL非常高，也就是说每次地震我们都希望预测出来。这个时候我们可以牺牲PRECISION。情愿发出1000次警报，把10次地震都预测正确了；也不要预测100次对了8次漏了两次。
2. 嫌疑人定罪基于不错怪一个好人的原则，对于嫌疑人的定罪我们希望是非常准确的。及时有时候放过了一些罪犯（recall低），但也是值得的。对于分类器来说，本质上是给一个概率，此时，我们再选择一个CUTOFF点（阀值），高于这个点的判正，低于的判负。那么这个点的选择就需要结合你的具体场景去选择。反过来，场景会决定训练模型时的标准，比如第一个场景中，我们就只看RECALL=99.9999%（地震全中）时的PRECISION，其他指标就变得没有了意义
# 各自的优缺点
## AUC (Area Under the ROC Curve)
- **优点：** 
  - 综合指标，可评估模型在不同阈值下的整体性能。
  - 对于类别不平衡问题较为稳健，不受类别分布影响。
  - 可用于比较不同模型性能，不受类别分布变化影响。
- **缺点：** 
  - 无法直接解释为实际概率或误差。
  - 对于类别分布均衡的问题，区分度可能不够敏感。

## Precision (精确率)
- **优点：** 
  - 重点关注模型预测为正例的样本中有多少是真正的正例。
  - 适用于需要关注正例的情况，如医疗诊断等。
  - 在一些场景下，假正例的影响较大，需要降低误报率。
- **缺点：** 
  - 不考虑漏报率，无法全面衡量模型的分类能力。
  - 在类别不平衡的情况下，如果某个类别样本较少，可能会偏高。

## Recall (召回率)
- **优点：** 
  - 强调模型对正例的覆盖能力。
  - 适用于需要捕捉所有正例的情况，如疾病诊断等。
  - 在一些场景下，漏报的影响较大，需要避免错过正例。
- **缺点：** 
  - 不考虑误报率，可能导致过多误报。
  - 在类别不平衡的情况下，如果某个类别样本较多，可能会偏高。

## F1 Score
- **优点：** 
  - Precision 和 Recall 的调和平均值，综合考虑两者性能。
  - 适用于在 Precision 和 Recall 之间需要权衡的情况。
- **缺点：** 
  - 对 Precision 和 Recall 给予相同权重，不适用于所有情况。
  - 在某些场景下，可能更关心其中之一。
