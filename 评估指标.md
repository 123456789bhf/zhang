# 评估指标

机器学习的评价指标有精度、精确率、召回率、P-R曲线、F1 值、TPR、FPR、ROC、AUC等指标，还有在生物领域常用的敏感性、特异性等指标。

## 基础

在分类任务中，各指标的计算基础都来自于对正负样本的分类结果，用混淆矩阵表示，如 **图1** 所示：

<center><img src="https://raw.githubusercontent.com/w5688414/paddleImage/main/metrics_img/confusion_metric.png" width="500" hegiht="" ></center>
<center><br>图1 混淆矩阵 </br></center>

## 精度


$$Accuracy=\frac{TP+TN}{TP+FN+FP+TN}$$

即所有分类正确的样本占全部样本的比例。

## 精确率
精准率又叫做：Precision、查准率

$$Precision=\frac{TP}{TP+FP}$$

即预测是正例的结果中，确实是正例的比例。

## 召回率

召回率又叫：Recall、查全率

$$Recall=\frac{TP}{TP+FN}$$

即所有正例的样本中，被找出的比例

召回率的应用场景：比如望月违约率为例，相对好拥护，我们更关心坏用户，不能错放过任何一个坏用户。因为如果我们果果的坏用户被当成好用户，这样后续发生的违约金额会远远超过好用户仓换的借款利息金额，造成严重的损失。召回率越高，代表实际坏用户被检测出来的概率越高，它的含义是：**宁可错杀一千，也不放过过一个**

### 精准率与召回率缺点
- 因为很多都是分类问题，所以在于预测的时候我们需要设置一个阈值，如果输出大于这个阈值，那么将其分为正lei，否则，父类，这样对于不同的阈值对应着不同的精准率与召回率，寻找最佳的阈值比较酷男
- 什么是最好的阈值点：我们对这两指标的要求：希望精准率与召回率越高越好，但实际上这是相矛盾的，无法做到双高。在p-R途中表明如果一个非常高，另一个一定非常低。选取合适的阈值点要根据实际的要求，比如我们想要搞得召回率就要牺牲一些靳准率


## P-R曲线

P-R曲线又叫做：PRC

<center><img src="https://raw.githubusercontent.com/w5688414/paddleImage/main/metrics_img/PRC.png" width="500" hegiht="" ></center>
<center><br>图2 PRC曲线图</br></center>

根据预测结果将预测样本排序，最有可能为正样本的在前，最不可能的在后，依次将样本预测为正样本，分别计算当前的精确率和召回率，绘制P-R曲线。

- 进一步分析

<center><img src="https://pic1.zhimg.com/v2-fa55ae6a09aefc608880ccc3f4ce2140_r.jpg" width="500" hegiht="" ></center>
<center><br>图2 PRC曲线图</br></center>

- 从上图不难发现，precision与Recall的折中(trade off)，曲线越靠近右上角性能越好，曲线下的面积叫AP分数，能在一定程度上反应模型的精确率和召回率都很高的比例。但这个值不方便计算，综合考虑精度与召回率一般使用F1函数或者AUC值（因为ROC曲线很容易画，ROC曲线下的面积也比较容易计算）.
- 
- 分析方法
  - 先看平滑不平滑，在看谁上谁下（同一测试集上），一般来说，上面的比下面的好（红线比黑线好）；
  - F1（计算公式略）当P和R接近就也越大，一般会画连接(0,0)和(1,1)的线，线和PRC重合的地方的就是F1,F1考虑了精准率与召回率，让两者进行了一个平衡，都比较高


## F1 值

$$F1=\frac{2 * P * R}{P + R}$$

### 优点
F1考虑了精准率与召回率，让两者进行了一个平衡，都比较高
### 缺点
阈值需要自己选取

## TPR

真正例率，与召回率相同

$$TPR=\frac{TP}{TP+FN}$$

## FPR

假正例率

$$FPR=\frac{FP}{TN+FP}$$

## ROC

受试者工作特征

根据预测结果将预测样本排序，最有可能为正样本的在前，最不可能的在后，依次将样本预测为正样本，分别计算当前的TPR和FPR，绘制ROC曲线。

## AUC

Area Under ROC Curve，ROC曲线下的面积：

<center><img src="https://raw.githubusercontent.com/w5688414/paddleImage/main/metrics_img/AUC.png" width="500" hegiht="" ></center>
<center><br>图3 ROC曲线图</br></center>

### 进一步分析
1. 什么是AUC

AUC是而分类模型的评价指标，对于二分类模型，还有其他的评价指标，比如logloss,accuracy,precision,recall,但是较常用的是AUC,Logloss,因为很对机器学习的模型对分类问题的预测结果都是概率，如果要计算accuracy,需要将概率转换为类别，这就需要手动设置一个阈值，如果一个样本的预测概率高于这个阈值，那么就把这个样本放进这个里面，低于这个与之，放进零一个类别里面。使用AUC或者logloaa就可以避免将概率转换为类别。

2. AUC的具体定义
   
即AUC是指随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值比分类器输出该负样本为正的那个概率值要大的可能性。所以AUC反应的是分类器对样本的排序能力。

3. AUC的优点

根据这个解释，如果我们完全随机的对样本分类，那么AUC应该接近0.5。（所以一般训练出的模型，AUC>0.5,如果AUC=0.5，这个分类器等于没有效果，效果与完全随机一样，如果AUC<0.5，则可能是标签标注错误等情况造成）；

另外值得注意的是，AUC的计算方法同时考虑了学习器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器做出合理的评价。AUC对样本类别是否均衡并不敏感，这也是不均衡样本通常用AUC评价学习器性能的一个原因。

4. AUC的计算方法

AUC的计算方法有多种，从物理意义角度理解，AUC计算的是ROC曲线下的面积：
见链接：https://zhuanlan.zhihu.com/p/43405406




## 敏感性

敏感性或者灵敏度（Sensitivity，也称为真阳性率）是指实际为阳性的样本中，判断为阳性的比例（例如真正有生病的人中，被医院判断为有生病者的比例），计算方式是真阳性除以真阳性+假阴性（实际为阳性，但判断为阴性）的比值（能将实际患病的病例正确地判断为患病的能力，即患者被判为阳性的概率）。公式如下：

$$sensitivity =\frac{TP}{TP + FN}$$

即有病（阳性）人群中，检测出阳性的几率。（检测出确实有病的能力）

## 特异性

特异性或特异度（Specificity，也称为真阴性率）是指实际为阴性的样本中，判断为阴性的比例（例如真正未生病的人中，被医院判断为未生病者的比例），计算方式是真阴性除以真阴性+假阳性（实际为阴性，但判断为阳性）的比值（能正确判断实际未患病的病例的能力，即试验结果为阴性的比例）。公式如下：

$$specificity =\frac{TN}{TN + FP}$$

即无病（阴性）人群中，检测出阴性的几率。（检测出确实没病的能力）
## 具体的应用场景
1. 地震的预测对于地震的预测，我们希望的是RECALL非常高，也就是说每次地震我们都希望预测出来。这个时候我们可以牺牲PRECISION。情愿发出1000次警报，把10次地震都预测正确了；也不要预测100次对了8次漏了两次。
2. 嫌疑人定罪基于不错怪一个好人的原则，对于嫌疑人的定罪我们希望是非常准确的。及时有时候放过了一些罪犯（recall低），但也是值得的。对于分类器来说，本质上是给一个概率，此时，我们再选择一个CUTOFF点（阀值），高于这个点的判正，低于的判负。那么这个点的选择就需要结合你的具体场景去选择。反过来，场景会决定训练模型时的标准，比如第一个场景中，我们就只看RECALL=99.9999%（地震全中）时的PRECISION，其他指标就变得没有了意义
